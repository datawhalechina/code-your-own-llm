{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“¦ NanoChat æ•°æ®å¤„ç†æŒ‡å—\n",
    "\n",
    "> **å†™ç»™å°ç™½çš„è¯**ï¼šè¿™ä¸ª Notebook ä¼šæ‰‹æŠŠæ‰‹æ•™ä½ å¦‚ä½•å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä¸éœ€è¦ä»»ä½•ä¸“ä¸šèƒŒæ™¯ï¼Œè·Ÿç€è¿è¡Œæ¯ä¸ªå•å…ƒæ ¼å°±è¡Œï¼\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ç›®å½•\n",
    "\n",
    "1. [æ ¸å¿ƒæ¦‚å¿µï¼š3 åˆ†é’Ÿå¿«é€Ÿç†è§£](#æ ¸å¿ƒæ¦‚å¿µ)\n",
    "2. [ç¬¬ä¸€é˜¶æ®µï¼šé¢„è®­ç»ƒæ•°æ®](#é¢„è®­ç»ƒæ•°æ®)\n",
    "3. [ç¬¬äºŒé˜¶æ®µï¼šä¸­æœŸè®­ç»ƒæ•°æ®](#ä¸­æœŸè®­ç»ƒæ•°æ®)\n",
    "4. [ç¬¬ä¸‰é˜¶æ®µï¼šå¾®è°ƒæ•°æ®](#å¾®è°ƒæ•°æ®)\n",
    "5. [å®æˆ˜ï¼šå‡†å¤‡ä¸­æ–‡æ•°æ®](#å‡†å¤‡ä¸­æ–‡æ•°æ®)\n",
    "6. [æ•°æ®è´¨é‡æ£€æŸ¥å·¥å…·](#æ•°æ®è´¨é‡æ£€æŸ¥)\n",
    "7. [æ•°æ®é‡è®¡ç®—å™¨](#æ•°æ®é‡è®¡ç®—å™¨)\n",
    "8. [å®Œæ•´æµç¨‹æ£€æŸ¥æ¸…å•](#æ£€æŸ¥æ¸…å•)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"æ ¸å¿ƒæ¦‚å¿µ\"></a>1. æ ¸å¿ƒæ¦‚å¿µï¼š3 åˆ†é’Ÿå¿«é€Ÿç†è§£\n",
    "\n",
    "### è®­ç»ƒ AI éœ€è¦ä»€ä¹ˆæ•°æ®ï¼Ÿ\n",
    "\n",
    "æƒ³è±¡ä¸€ä¸‹æ•™å°å­©å­¦è¯´è¯çš„è¿‡ç¨‹ï¼š\n",
    "\n",
    "```\n",
    "ğŸ‘¶ ç¬¬ä¸€é˜¶æ®µï¼šå¬å¤§é‡æ—¥å¸¸å¯¹è¯ â†’ å­¦ä¼šåŸºæœ¬è¯­è¨€èƒ½åŠ›\n",
    "ğŸ‘§ ç¬¬äºŒé˜¶æ®µï¼šå­¦ä¹ é—®ç­”æ–¹å¼ â†’ æ‡‚å¾—å¯¹è¯ç»“æ„  \n",
    "ğŸ‘¨ ç¬¬ä¸‰é˜¶æ®µï¼šå­¦ä¹ å›ç­”é—®é¢˜ â†’ èƒ½æŒ‰è¦æ±‚å›ç­”\n",
    "```\n",
    "\n",
    "è®­ç»ƒ AI æ¨¡å‹ä¹Ÿæ˜¯ä¸€æ ·çš„ **ä¸‰ä¸ªé˜¶æ®µ**ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# åˆ›å»ºè®­ç»ƒé˜¶æ®µå¯¹æ¯”è¡¨\n",
    "training_stages = pd.DataFrame({\n",
    "    'é˜¶æ®µ': ['1ï¸âƒ£', '2ï¸âƒ£', '3ï¸âƒ£'],\n",
    "    'åç§°': ['é¢„è®­ç»ƒ (Pretraining)', 'ä¸­æœŸè®­ç»ƒ (Midtraining)', 'å¾®è°ƒ (Fine-tuning)'],\n",
    "    'æ•°æ®ç±»å‹': ['æµ·é‡ç½‘é¡µæ–‡æœ¬', 'å¯¹è¯è®°å½•', 'æŒ‡ä»¤å¯¹è¯å¯¹'],\n",
    "    'å­¦ä»€ä¹ˆ': ['è¯­è¨€çš„åŸºæœ¬è§„å¾‹ã€è¯­æ³•ã€è¯æ±‡ã€å¸¸è¯†', 'å¯¹è¯çš„æ ¼å¼ã€ä¸€é—®ä¸€ç­”çš„ç»“æ„', 'ç†è§£å’Œæ‰§è¡ŒæŒ‡ä»¤ã€åšä¸ªå¥½åŠ©æ‰‹'],\n",
    "    'æ•°æ®é‡': ['è¶…çº§å¤§ (å‡ å GB)', 'ä¸­ç­‰ (å‡ ç™¾ MB)', 'è¾ƒå° (å‡ å MB)']\n",
    "})\n",
    "\n",
    "print(\"\\nğŸ¯ AI è®­ç»ƒçš„ä¸‰ä¸ªé˜¶æ®µ\\n\")\n",
    "display(training_stages)\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ ä¸ºä»€ä¹ˆè¦åˆ†ä¸‰ä¸ªé˜¶æ®µï¼Ÿ\n",
    "\n",
    "**ç±»æ¯”ï¼šå°±åƒå­¦è‹±è¯­**\n",
    "\n",
    "- **é¢„è®­ç»ƒ** = å¤§é‡é˜…è¯»è‹±æ–‡ä¹¦ç±ï¼ˆå­¦è¯­æ³•å’Œè¯æ±‡ï¼‰\n",
    "- **ä¸­æœŸè®­ç»ƒ** = å­¦ä¹ è‹±è¯­å¯¹è¯ï¼ˆå­¦æ€ä¹ˆäº¤æµï¼‰\n",
    "- **å¾®è°ƒ** = å­¦ä¹ å›ç­”é¢è¯•é—®é¢˜ï¼ˆå­¦ç‰¹å®šä»»åŠ¡ï¼‰\n",
    "\n",
    "å¦‚æœç›´æ¥è®© AI å­¦ä¹ å›ç­”é—®é¢˜è€Œä¸å…ˆå­¦è¯­è¨€ï¼Œå°±åƒè®©å®Œå…¨ä¸æ‡‚è‹±è¯­çš„äººç›´æ¥å‚åŠ è‹±è¯­é¢è¯•ï¼Œè‚¯å®šå­¦ä¸å¥½ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"é¢„è®­ç»ƒæ•°æ®\"></a>2. ç¬¬ä¸€é˜¶æ®µï¼šé¢„è®­ç»ƒæ•°æ®\n",
    "\n",
    "### ç”¨ä»€ä¹ˆæ•°æ®ï¼Ÿ\n",
    "\n",
    "é¡¹ç›®é»˜è®¤ä½¿ç”¨ **FineWeb-Edu** æ•°æ®é›†ï¼š\n",
    "\n",
    "- ğŸ“– æ¥æºï¼š**Datawhale/fineweb-edu-100b-shuffle**ï¼ˆModelScope å¹³å°ï¼‰\n",
    "- ğŸ”— è®¿é—®åœ°å€ï¼š[https://modelscope.cn/datasets/Datawhale/fineweb-edu-100b-shuffle](https://modelscope.cn/datasets/Datawhale/fineweb-edu-100b-shuffle)\n",
    "- ğŸ“Š è§„æ¨¡ï¼šçº¦ 1000 äº¿ä¸ªå•è¯ï¼ˆæ˜¯çš„ï¼Œ1000 äº¿ï¼ï¼‰\n",
    "- âœ¨ è´¨é‡ï¼šé«˜è´¨é‡ç½‘é¡µå†…å®¹ï¼Œå·²ç»è¿‡æ··æ´—å¤„ç†\n",
    "- ğŸ å…è´¹ï¼šå®Œå…¨å¼€æºï¼Œç›´æ¥ä¸‹è½½\n",
    "- ğŸš€ **å›½å†…ä¼˜åŠ¿**ï¼šä» ModelScope ä¸‹è½½ï¼Œå›½å†…è®¿é—®é€Ÿåº¦æ›´å¿«æ›´ç¨³å®š\n",
    "\n",
    "### ğŸ“Š æˆ‘éœ€è¦ä¸‹è½½å¤šå°‘æ•°æ®ï¼Ÿ\n",
    "\n",
    "å–å†³äºä½ è¦è®­ç»ƒå¤šå¤§çš„æ¨¡å‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸åŒæ¨¡å‹è§„æ¨¡çš„æ•°æ®éœ€æ±‚å¯¹æ¯”è¡¨\n",
    "data_requirements = pd.DataFrame({\n",
    "    'æ¨¡å‹è§„æ¨¡': ['d10 (è¿·ä½ )', 'd12 (å°)', 'd20 (é»˜è®¤)', 'd26 (å¤§)', 'd32 (è¶…å¤§)'],\n",
    "    'å‚æ•°é‡': ['42M', '123M', '561M', '1.2B', '2.1B'],\n",
    "    'éœ€è¦ä¸‹è½½': ['16 ä¸ªåˆ†ç‰‡', '48 ä¸ªåˆ†ç‰‡', '215 ä¸ªåˆ†ç‰‡', '460 ä¸ªåˆ†ç‰‡', '806 ä¸ªåˆ†ç‰‡'],\n",
    "    'ç£ç›˜ç©ºé—´': ['~2GB', '~5GB', '~21GB', '~45GB', '~79GB'],\n",
    "    'è®­ç»ƒæ—¶é—´': ['30 åˆ†é’Ÿ', '1-2 å°æ—¶', '4 å°æ—¶', '12 å°æ—¶', '24 å°æ—¶']\n",
    "})\n",
    "\n",
    "print(\"\\nğŸ“Š æ¨¡å‹è§„æ¨¡ä¸æ•°æ®éœ€æ±‚å¯¹ç…§è¡¨\\n\")\n",
    "display(data_requirements)\n",
    "print(\"\\nğŸ’¡ æ–°æ‰‹å»ºè®®ï¼šå…ˆç”¨ d10 æˆ– d12 ç»ƒæ‰‹ï¼Œç†Ÿæ‚‰æµç¨‹åå†è®­ç»ƒå¤§æ¨¡å‹ï¼\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ å¦‚ä½•ä¸‹è½½ï¼Ÿ\n",
    "\n",
    "**ä¸€æ¡å‘½ä»¤æå®šï¼** è¿è¡Œä¸‹é¢çš„ä»£ç å•å…ƒæ ¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½ 8 ä¸ªåˆ†ç‰‡ç”¨äºè®­ç»ƒåˆ†è¯å™¨ï¼ˆçº¦ 800MBï¼‰\n",
    "# è¿™æ˜¯æœ€å°ä¸‹è½½é‡ï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•\n",
    "\n",
    "!python -m nanochat.dataset -n 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¦‚æœè¦è®­ç»ƒ d20 æ¨¡å‹ï¼Œéœ€è¦ä¸‹è½½æ›´å¤šæ•°æ®\n",
    "# âš ï¸ è­¦å‘Šï¼šè¿™ä¼šä¸‹è½½çº¦ 21GB æ•°æ®ï¼Œéœ€è¦è¾ƒé•¿æ—¶é—´ï¼\n",
    "# å¦‚æœä¸éœ€è¦ï¼Œè¯·ä¸è¦è¿è¡Œè¿™ä¸ªå•å…ƒæ ¼\n",
    "\n",
    "# !python -m nanochat.dataset -n 215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ æ•°æ®ä¸‹è½½åˆ°å“ªäº†ï¼Ÿ\n",
    "\n",
    "æ‰€æœ‰æ•°æ®è‡ªåŠ¨ä¿å­˜åˆ° `~/.cache/nanochat/base_data/`\n",
    "\n",
    "è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# è·å–æ•°æ®ç›®å½•\n",
    "data_dir = Path.home() / \".cache\" / \"nanochat\" / \"base_data\"\n",
    "\n",
    "print(f\"ğŸ“ æ•°æ®ç›®å½•: {data_dir}\\n\")\n",
    "\n",
    "if data_dir.exists():\n",
    "    # ç»Ÿè®¡å·²ä¸‹è½½çš„æ–‡ä»¶\n",
    "    parquet_files = list(data_dir.glob(\"*.parquet\"))\n",
    "    \n",
    "    if parquet_files:\n",
    "        print(f\"âœ… æ‰¾åˆ° {len(parquet_files)} ä¸ªæ•°æ®æ–‡ä»¶\")\n",
    "        \n",
    "        # è®¡ç®—æ€»å¤§å°\n",
    "        total_size = sum(f.stat().st_size for f in parquet_files)\n",
    "        print(f\"ğŸ’½ æ€»å¤§å°: {total_size / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # æ˜¾ç¤ºå‰ 5 ä¸ªæ–‡ä»¶\n",
    "        print(\"\\nå‰ 5 ä¸ªæ–‡ä»¶:\")\n",
    "        for f in sorted(parquet_files)[:5]:\n",
    "            size_mb = f.stat().st_size / (1024**2)\n",
    "            print(f\"  ğŸ“„ {f.name:25s} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(\"âš ï¸ æ•°æ®ç›®å½•å­˜åœ¨ï¼Œä½†æ²¡æœ‰æ‰¾åˆ° .parquet æ–‡ä»¶\")\n",
    "        print(\"   è¯·å…ˆè¿è¡Œä¸Šé¢çš„ä¸‹è½½å‘½ä»¤ï¼\")\n",
    "else:\n",
    "    print(\"âš ï¸ æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆä¸‹è½½æ•°æ®ï¼\")\n",
    "    print(f\"   è¿è¡Œ: python -m nanochat.dataset -n 8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” æŸ¥çœ‹æ•°æ®å†…å®¹\n",
    "\n",
    "è®©æˆ‘ä»¬æ‰“å¼€ä¸€ä¸ªæ–‡ä»¶çœ‹çœ‹é‡Œé¢æ˜¯ä»€ä¹ˆï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# è¯»å–ç¬¬ä¸€ä¸ªåˆ†ç‰‡\n",
    "data_dir = Path.home() / \".cache\" / \"nanochat\" / \"base_data\"\n",
    "parquet_files = list(data_dir.glob(\"*.parquet\")) if data_dir.exists() else []\n",
    "\n",
    "if parquet_files:\n",
    "    first_file = sorted(parquet_files)[0]\n",
    "    print(f\"ğŸ“– æ­£åœ¨è¯»å–: {first_file.name}\\n\")\n",
    "    \n",
    "    # è¯»å– Parquet æ–‡ä»¶\n",
    "    table = pq.read_table(first_file)\n",
    "    \n",
    "    print(f\"ğŸ“Š æ–‡ä»¶ä¿¡æ¯:\")\n",
    "    print(f\"   è¡Œæ•°: {len(table):,}\")\n",
    "    print(f\"   åˆ—å: {table.column_names}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºå‰ 3 æ¡æ•°æ®\n",
    "    print(\"\\nğŸ“ å‰ 3 æ¡æ•°æ®ç¤ºä¾‹:\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i in range(min(3, len(table))):\n",
    "        text = table['text'][i].as_py()\n",
    "        # åªæ˜¾ç¤ºå‰ 200 ä¸ªå­—ç¬¦\n",
    "        preview = text[:200] + \"...\" if len(text) > 200 else text\n",
    "        print(f\"\\nç¬¬ {i+1} æ¡ (é•¿åº¦: {len(text)} å­—ç¬¦)\")\n",
    "        print(\"-\" * 80)\n",
    "        print(preview)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"âš ï¸ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆä¸‹è½½æ•°æ®ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’» æ•°æ®ä¸‹è½½ä»£ç è¯¦è§£\n",
    "\n",
    "æ•°æ®ä¸‹è½½åŠŸèƒ½ç”± `nanochat/dataset.py` å®ç°ï¼Œæ ¸å¿ƒä»£ç å¦‚ä¸‹ï¼š\n",
    "\n",
    "**æ ¸å¿ƒåŠŸèƒ½ï¼š**\n",
    "1. **å¤šè¿›ç¨‹å¹¶è¡Œä¸‹è½½**ï¼šé»˜è®¤ä½¿ç”¨ 4 ä¸ªè¿›ç¨‹åŒæ—¶ä¸‹è½½\n",
    "2. **è‡ªåŠ¨é‡è¯•æœºåˆ¶**ï¼šä¸‹è½½å¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•ï¼Œæœ€å¤š 5 æ¬¡\n",
    "3. **æ–­ç‚¹ç»­ä¼ **ï¼šå·²ä¸‹è½½çš„æ–‡ä»¶ä¼šè‡ªåŠ¨è·³è¿‡\n",
    "4. **ä¸´æ—¶æ–‡ä»¶ä¿æŠ¤**ï¼šå…ˆä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œå®Œæˆåæ‰é‡å‘½åï¼Œé¿å…ä¸­æ–­å¯¼è‡´æ–‡ä»¶æŸå\n",
    "\n",
    "**æ•°æ®æºé…ç½®ï¼š**\n",
    "- é»˜è®¤ä½¿ç”¨ ModelScopeï¼š`Datawhale/fineweb-edu-100b-shuffle`\n",
    "- å›½å†…è®¿é—®é€Ÿåº¦å¿«ï¼Œæ— éœ€ç‰¹æ®Šé…ç½®\n",
    "- æ€»å…± 1822 ä¸ªåˆ†ç‰‡ï¼Œæ¯ä¸ªçº¦ 100MB\n",
    "\n",
    "**å…³é”®ä»£ç ä½ç½®ï¼š**\n",
    "- æ•°æ®æºé…ç½®ï¼š```nanochat/dataset.py```\n",
    "- ä¸‹è½½å‡½æ•°ï¼š```nanochat/dataset.py```\n",
    "- ä¸»ç¨‹åºï¼š```nanochat/dataset.py```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹ dataset.py çš„å…³é”®ä»£ç \n",
    "# å®Œæ•´ä»£ç åœ¨: nanochat/dataset.py\n",
    "\n",
    "print(\"ğŸ“„ æ•°æ®ä¸‹è½½æ¨¡å—æ ¸å¿ƒä»£ç ï¼š\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# æ•°æ®æºé…ç½®ï¼ˆå·²ä¼˜åŒ–ä¸ºå›½å†…æºï¼‰\n",
    "# å®Œæ•´ä»£ç ï¼šnanochat/dataset.py (ç¬¬ 22-29 è¡Œ)\n",
    "print(\"\"\"\n",
    "# æ•°æ®æºé…ç½®ï¼ˆå·²ä¼˜åŒ–ä¸ºå›½å†…æºï¼‰\n",
    "BASE_URL = \"https://modelscope.cn/api/v1/datasets/Datawhale/fineweb-edu-100b-shuffle/repo?Revision=master&FilePath=\"\n",
    "MAX_SHARD = 1822  # æ€»å…± 1822 ä¸ªåˆ†ç‰‡\n",
    "index_to_filename = lambda index: f\"shard_{index:05d}.parquet\"  # æ–‡ä»¶åæ ¼å¼\n",
    "DATA_DIR = os.path.join(base_dir, \"base_data\")  # æ•°æ®ä¿å­˜ç›®å½•\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# ä¸‹è½½å•ä¸ªæ–‡ä»¶çš„å‡½æ•°ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰\n",
    "# å®Œæ•´ä»£ç ï¼šnanochat/dataset.py (ç¬¬ 61-110 è¡Œ)\n",
    "print(\"\"\"\n",
    "def download_single_file(index):\n",
    "    \\\"\\\"\\\"ä¸‹è½½å•ä¸ªæ–‡ä»¶ï¼Œå¸¦é‡è¯•æœºåˆ¶\\\"\\\"\\\"\n",
    "    filename = index_to_filename(index)\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    \n",
    "    # å·²å­˜åœ¨åˆ™è·³è¿‡ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰\n",
    "    if os.path.exists(filepath):\n",
    "        return True\n",
    "    \n",
    "    url = f\"{BASE_URL}/{filename}\"\n",
    "    \n",
    "    # æœ€å¤šé‡è¯• 5 æ¬¡ï¼ŒæŒ‡æ•°é€€é¿\n",
    "    max_attempts = 5\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        try:\n",
    "            response = requests.get(url, stream=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # å…ˆå†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼ˆä¸´æ—¶æ–‡ä»¶ä¿æŠ¤ï¼‰\n",
    "            temp_path = filepath + \".tmp\"\n",
    "            with open(temp_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024*1024):  # 1MB chunks\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            \n",
    "            # åŸå­æ“ä½œï¼šé‡å‘½åä¸´æ—¶æ–‡ä»¶\n",
    "            os.rename(temp_path, filepath)\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            # æ¸…ç†éƒ¨åˆ†æ–‡ä»¶\n",
    "            if attempt < max_attempts:\n",
    "                wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                return False\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# å¤šè¿›ç¨‹å¹¶è¡Œä¸‹è½½\n",
    "# å®Œæ•´ä»£ç ï¼šnanochat/dataset.py (ç¬¬ 113-129 è¡Œ)\n",
    "print(\"\"\"\n",
    "# ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œä¸‹è½½ï¼ˆé»˜è®¤ 4 ä¸ªè¿›ç¨‹ï¼‰\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-n\", \"--num-files\", type=int, default=-1)\n",
    "parser.add_argument(\"-w\", \"--num-workers\", type=int, default=4)\n",
    "args = parser.parse_args()\n",
    "\n",
    "ids_to_download = list(range(num))\n",
    "with Pool(processes=args.num_workers) as pool:\n",
    "    results = pool.map(download_single_file, ids_to_download)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nğŸ’¡ å®Œæ•´ä»£ç è¯·æŸ¥çœ‹: nanochat/dataset.py\")\n",
    "print(\"   å…³é”®ç‰¹æ€§ï¼šå¤šè¿›ç¨‹ã€è‡ªåŠ¨é‡è¯•ã€æ–­ç‚¹ç»­ä¼ ã€ä¸´æ—¶æ–‡ä»¶ä¿æŠ¤\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"ä¸­æœŸè®­ç»ƒæ•°æ®\"></a>3. ç¬¬äºŒé˜¶æ®µï¼šä¸­æœŸè®­ç»ƒæ•°æ®\n",
    "\n",
    "### ç”¨ä»€ä¹ˆæ•°æ®ï¼Ÿ\n",
    "\n",
    "é¡¹ç›®é»˜è®¤ä½¿ç”¨ **SmolTalk** å¯¹è¯æ•°æ®é›†ï¼š\n",
    "\n",
    "#### ğŸŒ æ•°æ®æºä¿¡æ¯\n",
    "\n",
    "- ğŸ“– æ•°æ®é›†ï¼š`HuggingFaceTB/smoltalk`\n",
    "- ğŸ¢ å¹³å°ï¼šHuggingFace\n",
    "- ğŸ—£ï¸ å†…å®¹ï¼šçœŸå®çš„äººç±»å¯¹è¯è®°å½•\n",
    "- ğŸ“ æ ¼å¼ï¼šä¸€é—®ä¸€ç­”çš„å¯¹è¯å½¢å¼\n",
    "- ğŸ¯ ç›®çš„ï¼šè®©æ¨¡å‹å­¦ä¼šå¯¹è¯çš„æ ¼å¼\n",
    "- ğŸ“¥ ä¸‹è½½æ–¹å¼ï¼šè®­ç»ƒè„šæœ¬è‡ªåŠ¨ä¸‹è½½\n",
    "\n",
    "#### ğŸ‡¨ğŸ‡³ å›½å†…è®¿é—®ä¼˜åŒ–\n",
    "\n",
    "å¦‚æœä¸‹è½½é€Ÿåº¦æ…¢ï¼Œå¯ä»¥è®¾ç½® HuggingFace é•œåƒåŠ é€Ÿï¼š\n",
    "\n",
    "```bash\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "```\n",
    "\n",
    "### æ•°æ®æ ¼å¼ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# å¯¹è¯æ•°æ®æ ¼å¼ç¤ºä¾‹\n",
    "dialogue_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"ä½ å¥½ï¼è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ª AI åŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”é—®é¢˜ã€æä¾›å»ºè®®...\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"ä½ ä¼šè¯´ä¸­æ–‡å—ï¼Ÿ\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"æ˜¯çš„ï¼Œæˆ‘å¯ä»¥ä½¿ç”¨ä¸­æ–‡äº¤æµã€‚\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"ğŸ“ å¯¹è¯æ•°æ®æ ¼å¼ç¤ºä¾‹ï¼š\\n\")\n",
    "print(json.dumps(dialogue_example, ensure_ascii=False, indent=2))\n",
    "\n",
    "print(\"\\nğŸ’¡ é‡è¦å­—æ®µè¯´æ˜ï¼š\")\n",
    "print(\"   â€¢ role: è¯´è¯çš„è§’è‰²ï¼Œ'user'(ç”¨æˆ·) æˆ– 'assistant'(åŠ©æ‰‹)\")\n",
    "print(\"   â€¢ content: è¯´è¯çš„å†…å®¹\")\n",
    "\n",
    "print(\"\\nâœ… å¥½æ¶ˆæ¯ï¼šè®­ç»ƒè„šæœ¬ä¼šè‡ªåŠ¨ä¸‹è½½ SmolTalk æ•°æ®é›†ï¼Œæ— éœ€æ‰‹åŠ¨æ“ä½œï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"å¾®è°ƒæ•°æ®\"></a>4. ç¬¬ä¸‰é˜¶æ®µï¼šå¾®è°ƒæ•°æ®\n",
    "\n",
    "### ç”¨ä»€ä¹ˆæ•°æ®ï¼Ÿ\n",
    "\n",
    "å¾®è°ƒé˜¶æ®µæ··åˆä½¿ç”¨å¤šä¸ªä»»åŠ¡æ•°æ®é›†ï¼š\n",
    "\n",
    "#### ğŸŒ æ•°æ®é›†åˆ—è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¾®è°ƒæ•°æ®é›†æ¦‚è§ˆ\n",
    "sft_datasets = pd.DataFrame({\n",
    "    'æ•°æ®é›†': ['ARC-Easy', 'ARC-Challenge', 'GSM8K', 'SmolTalk'],\n",
    "    'å†…å®¹': ['ç®€å•é€‰æ‹©é¢˜', 'å›°éš¾é€‰æ‹©é¢˜', 'å°å­¦æ•°å­¦é¢˜', 'æ—¥å¸¸å¯¹è¯'],\n",
    "    'æ•°é‡': ['2,300 æ¡', '1,100 æ¡', '8,000 æ¡', '10,000 æ¡'],\n",
    "    'å­¦ä»€ä¹ˆèƒ½åŠ›': ['å¸¸è¯†æ¨ç†', 'æ·±åº¦æ¨ç†', 'æ•°å­¦è®¡ç®—', 'é—²èŠèƒ½åŠ›']\n",
    "})\n",
    "\n",
    "print(\"\\nğŸ¯ å¾®è°ƒé˜¶æ®µçš„æ•°æ®é›†\\n\")\n",
    "display(sft_datasets)\n",
    "print(\"\\nğŸ“Š æ€»è®¡ï¼šçº¦ 21,400 æ¡è®­ç»ƒæ ·æœ¬\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ‡¨ğŸ‡³ å›½å†…è®¿é—®ä¼˜åŒ–\n",
    "\n",
    "æ‰€æœ‰å¾®è°ƒæ•°æ®é›†æ¥è‡ª HuggingFaceï¼Œä¼šåœ¨è®­ç»ƒæ—¶è‡ªåŠ¨ä¸‹è½½ã€‚å›½å†…ç”¨æˆ·å»ºè®®è®¾ç½®é•œåƒï¼š\n",
    "\n",
    "```bash\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "```\n",
    "\n",
    "æˆ–åœ¨ Python ä»£ç ä¸­è®¾ç½®ï¼š\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "```\n",
    "\n",
    "### æ•°æ®æ ¼å¼ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°å­¦é¢˜ç¤ºä¾‹ (GSM8K)\n",
    "math_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"å°æ˜æœ‰8ä¸ªè‹¹æœï¼Œåƒæ‰äº†3ä¸ªï¼Œè¿˜å‰©å‡ ä¸ªï¼Ÿ\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"è®©æˆ‘æ¥ç®—ä¸€ä¸‹ï¼š\\n8 - 3 = 5\\næ‰€ä»¥å°æ˜è¿˜å‰©5ä¸ªè‹¹æœã€‚\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# é€‰æ‹©é¢˜ç¤ºä¾‹ (ARC)\n",
    "arc_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"å“ªä¸ªç‰©ä½“ä¼šæµ®åœ¨æ°´é¢ä¸Šï¼Ÿ\\nA. çŸ³å¤´\\nB. é“é’‰\\nC. æœ¨å¤´\\nD. ç»ç’ƒçƒ\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"ç­”æ¡ˆæ˜¯C. æœ¨å¤´ã€‚å› ä¸ºæœ¨å¤´çš„å¯†åº¦æ¯”æ°´å°ï¼Œæ‰€ä»¥ä¼šæµ®åœ¨æ°´é¢ä¸Šã€‚\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"ğŸ“ æ•°å­¦é¢˜ç¤ºä¾‹ (GSM8K)ï¼š\\n\")\n",
    "print(json.dumps(math_example, ensure_ascii=False, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"ğŸ“ é€‰æ‹©é¢˜ç¤ºä¾‹ (ARC)ï¼š\\n\")\n",
    "print(json.dumps(arc_example, ensure_ascii=False, indent=2))\n",
    "\n",
    "print(\"\\nâœ… è¿™äº›æ•°æ®é›†ä¼šåœ¨è¿è¡Œå¾®è°ƒè„šæœ¬æ—¶è‡ªåŠ¨ä¸‹è½½ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"å‡†å¤‡ä¸­æ–‡æ•°æ®\"></a>5. å®æˆ˜ï¼šå‡†å¤‡ä¸­æ–‡æ•°æ®\n",
    "\n",
    "> å¦‚æœä½ æƒ³è®­ç»ƒä¸­æ–‡æ¨¡å‹ï¼Œéœ€è¦å‡†å¤‡ä¸­æ–‡æ•°æ®ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹ï¼\n",
    "\n",
    "### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ HuggingFace ä¸­æ–‡æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®é•œåƒï¼ˆå¯é€‰ï¼‰\n",
    "import os\n",
    "\n",
    "# é¡¹ç›®å·²é»˜è®¤ä½¿ç”¨ ModelScope ä¸‹è½½é¢„è®­ç»ƒæ•°æ®ï¼Œæ— éœ€é¢å¤–è®¾ç½®\n",
    "# ä»¥ä¸‹é•œåƒè®¾ç½®ä»…ç”¨äºå…¶ä»– HuggingFace æ•°æ®é›†ï¼ˆå¦‚ SmolTalkï¼‰\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "print(\"âœ… å·²è®¾ç½® HuggingFace é•œåƒï¼šhttps://hf-mirror.com\")\n",
    "print(\"   è¿™ä¼šåŠ é€Ÿå…¶ä»– HuggingFace æ•°æ®é›†çš„ä¸‹è½½é€Ÿåº¦\")\n",
    "print(\"\\nğŸ’¡ é¢„è®­ç»ƒæ•°æ®é»˜è®¤ä» ModelScope ä¸‹è½½ï¼Œå›½å†…è®¿é—®é€Ÿåº¦å·²ä¼˜åŒ–\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½ä¸­æ–‡ç»´åŸºç™¾ç§‘æ•°æ®ï¼ˆç¤ºä¾‹ï¼‰\n",
    "# âš ï¸ è­¦å‘Šï¼šè¿™ä¼šä¸‹è½½è¾ƒå¤§çš„æ•°æ®é›†ï¼Œéœ€è¦æ—¶é—´ï¼\n",
    "# å¦‚æœä¸éœ€è¦ï¼Œè¯·ä¸è¦è¿è¡Œè¿™ä¸ªå•å…ƒæ ¼\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"ğŸ“¥ æ­£åœ¨ä¸‹è½½ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼ˆå‰ 1000 æ¡ç”¨äºæ¼”ç¤ºï¼‰...\\n\")\n",
    "\n",
    "try:\n",
    "    # åªä¸‹è½½å‰ 1000 æ¡ç”¨äºæ¼”ç¤º\n",
    "    wiki = load_dataset(\n",
    "        \"wikipedia\",\n",
    "        \"20220301.zh\",  # ä¸­æ–‡ç‰ˆæœ¬\n",
    "        split=\"train[:1000]\",  # åªå–å‰ 1000 æ¡\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… æˆåŠŸä¸‹è½½ï¼š{len(wiki):,} æ¡æ•°æ®\\n\")\n",
    "    \n",
    "    # æ˜¾ç¤ºç¬¬ä¸€æ¡\n",
    "    print(\"ğŸ“ ç¬¬ä¸€æ¡æ•°æ®ç¤ºä¾‹ï¼š\")\n",
    "    print(\"=\"*80)\n",
    "    first_text = wiki[0]['text'][:300] + \"...\"\n",
    "    print(first_text)\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ä¸‹è½½å¤±è´¥ï¼š{e}\")\n",
    "    print(\"   å¯èƒ½éœ€è¦æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å°è¯•ä½¿ç”¨é•œåƒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ–¹æ³•äºŒï¼šè½¬æ¢è‡ªå·±çš„æ–‡æœ¬æ•°æ®\n",
    "\n",
    "å¦‚æœä½ æœ‰è‡ªå·±æ”¶é›†çš„ä¸­æ–‡æ–‡æœ¬ï¼Œå¯ä»¥ä½¿ç”¨é¡¹ç›®æä¾›çš„è½¬æ¢å·¥å…·ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨å†…ç½®å·¥å…·è½¬æ¢è‡ªå®šä¹‰æ•°æ®\n",
    "# è¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹ data_check/convert_custom_data.py\n",
    "\n",
    "print(\"ğŸ› ï¸ è½¬æ¢è‡ªå®šä¹‰æ–‡æœ¬æ•°æ®çš„æ­¥éª¤ï¼š\\n\")\n",
    "print(\"1. å‡†å¤‡ä½ çš„æ–‡æœ¬æ•°æ®ï¼ˆ.txt æ–‡ä»¶ï¼‰\")\n",
    "print(\"2. è¿è¡Œè½¬æ¢å‘½ä»¤ï¼š\")\n",
    "print(\"   python -m data_check.convert_custom_data\")\n",
    "print(\"\\næ”¯æŒçš„è¾“å…¥æ ¼å¼ï¼š\")\n",
    "print(\"   â€¢ å•ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€æ¡æ•°æ®ï¼‰\")\n",
    "print(\"   â€¢ ç›®å½•ï¼ˆåŒ…å«å¤šä¸ª .txt æ–‡ä»¶ï¼‰\")\n",
    "print(\"\\nè¯¦ç»†ä»£ç è¯·æŸ¥çœ‹ï¼šdata_check/convert_custom_data.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"æ•°æ®è´¨é‡æ£€æŸ¥\"></a>6. æ•°æ®è´¨é‡æ£€æŸ¥å·¥å…·\n",
    "\n",
    "é¡¹ç›®æä¾›äº†å®Œæ•´çš„æ•°æ®æ£€æŸ¥å·¥å…·é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®æ£€æŸ¥å·¥å…·æ¦‚è§ˆ\n",
    "tools = pd.DataFrame({\n",
    "    'å·¥å…·': [\n",
    "        'check_data.py',\n",
    "        'check_length_distribution.py',\n",
    "        'check_content_quality.py',\n",
    "        'check_char_distribution.py',\n",
    "        'convert_custom_data.py'\n",
    "    ],\n",
    "    'ç”¨é€”': [\n",
    "        'éªŒè¯æ•°æ®æ–‡ä»¶å®Œæ•´æ€§',\n",
    "        'æ£€æŸ¥æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ',\n",
    "        'æŠ½æ ·æ£€æŸ¥å†…å®¹è´¨é‡',\n",
    "        'æ£€æŸ¥å­—ç¬¦åˆ†å¸ƒç»Ÿè®¡',\n",
    "        'è½¬æ¢è‡ªå®šä¹‰æ–‡æœ¬æ•°æ®'\n",
    "    ],\n",
    "    'å‘½ä»¤': [\n",
    "        'python -m data_check.check_data',\n",
    "        'python -m data_check.check_length_distribution',\n",
    "        'python -m data_check.check_content_quality',\n",
    "        'python -m data_check.check_char_distribution',\n",
    "        'python -m data_check.convert_custom_data'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nğŸ› ï¸ æ•°æ®æ£€æŸ¥å·¥å…·æ€»è§ˆ\\n\")\n",
    "display(tools)\n",
    "print(\"\\nğŸ’¡ æ‰€æœ‰å·¥å…·çš„è¯¦ç»†ä»£ç éƒ½åœ¨ data_check/ ç›®å½•ä¸‹\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¿«é€Ÿæ£€æŸ¥æ•°æ®å®Œæ•´æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥\n",
    "!python -m data_check.check_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ£€æŸ¥æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†ææ•°æ®çš„é•¿åº¦åˆ†å¸ƒ\n",
    "# è¿™æœ‰åŠ©äºäº†è§£æ•°æ®è´¨é‡\n",
    "\n",
    "!python -m data_check.check_length_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"æ•°æ®é‡è®¡ç®—å™¨\"></a>7. æ•°æ®é‡è®¡ç®—å™¨\n",
    "\n",
    "### Chinchilla å®šå¾‹\n",
    "\n",
    "**æ•°æ® token æ•° = æ¨¡å‹å‚æ•°é‡ Ã— 20**\n",
    "\n",
    "è®©æˆ‘ä»¬è®¡ç®—ä¸åŒæ¨¡å‹éœ€è¦å¤šå°‘æ•°æ®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–æ•°æ®é‡å¯¹æ¯”\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# å‡†å¤‡æ•°æ®\n",
    "models_list = [r['æ¨¡å‹'] for r in results]\n",
    "params_list = [float(r['å‚æ•°é‡'].replace('M', '')) for r in results]\n",
    "tokens_list = [float(r['Tokenæ•°'].replace('B', '')) for r in results]\n",
    "disk_list = [float(r['ç£ç›˜'].replace('GB', '')) for r in results]\n",
    "shards_list = [r['åˆ†ç‰‡æ•°'] for r in results]\n",
    "\n",
    "# åˆ›å»ºå¤šå­å›¾\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('ğŸ“Š ä¸åŒæ¨¡å‹è§„æ¨¡çš„æ•°æ®éœ€æ±‚å¯¹æ¯”', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. å‚æ•°é‡ vs Tokenæ•°\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(params_list, tokens_list, 'o-', linewidth=2, markersize=8, color='#4CAF50')\n",
    "ax1.set_xlabel('æ¨¡å‹å‚æ•°é‡ (M)', fontsize=12)\n",
    "ax1.set_ylabel('éœ€è¦çš„ Token æ•° (B)', fontsize=12)\n",
    "ax1.set_title('å‚æ•°é‡ vs Tokenæ•°ï¼ˆChinchilla å®šå¾‹ï¼‰', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "for i, model in enumerate(models_list):\n",
    "    ax1.annotate(model, (params_list[i], tokens_list[i]), \n",
    "                textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=10)\n",
    "\n",
    "# 2. Tokenæ•° vs ç£ç›˜ç©ºé—´\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(tokens_list, disk_list, 's-', linewidth=2, markersize=8, color='#2196F3')\n",
    "ax2.set_xlabel('Token æ•° (B)', fontsize=12)\n",
    "ax2.set_ylabel('ç£ç›˜ç©ºé—´ (GB)', fontsize=12)\n",
    "ax2.set_title('Tokenæ•° vs ç£ç›˜ç©ºé—´', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "for i, model in enumerate(models_list):\n",
    "    ax2.annotate(model, (tokens_list[i], disk_list[i]), \n",
    "                textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=10)\n",
    "\n",
    "# 3. åˆ†ç‰‡æ•°å¯¹æ¯”ï¼ˆæŸ±çŠ¶å›¾ï¼‰\n",
    "ax3 = axes[1, 0]\n",
    "bars = ax3.bar(models_list, shards_list, color=['#FF9800', '#F44336', '#9C27B0', '#00BCD4', '#4CAF50'], alpha=0.7)\n",
    "ax3.set_xlabel('æ¨¡å‹è§„æ¨¡', fontsize=12)\n",
    "ax3.set_ylabel('åˆ†ç‰‡æ•°', fontsize=12)\n",
    "ax3.set_title('ä¸åŒæ¨¡å‹éœ€è¦çš„åˆ†ç‰‡æ•°', fontsize=13, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "# åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for bar, shard in zip(bars, shards_list):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{shard}',\n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 4. ç£ç›˜ç©ºé—´å¯¹æ¯”ï¼ˆæŸ±çŠ¶å›¾ï¼‰\n",
    "ax4 = axes[1, 1]\n",
    "bars2 = ax4.bar(models_list, disk_list, color=['#FF9800', '#F44336', '#9C27B0', '#00BCD4', '#4CAF50'], alpha=0.7)\n",
    "ax4.set_xlabel('æ¨¡å‹è§„æ¨¡', fontsize=12)\n",
    "ax4.set_ylabel('ç£ç›˜ç©ºé—´ (GB)', fontsize=12)\n",
    "ax4.set_title('ä¸åŒæ¨¡å‹éœ€è¦çš„ç£ç›˜ç©ºé—´', fontsize=13, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "# åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for bar, disk in zip(bars2, disk_list):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{disk:.1f}GB',\n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_data_requirement(model_params_million):\n",
    "    \"\"\"\n",
    "    è®¡ç®—è®­ç»ƒæ‰€éœ€çš„æ•°æ®é‡\n",
    "    \n",
    "    å‚æ•°:\n",
    "        model_params_million: æ¨¡å‹å‚æ•°é‡(ç™¾ä¸‡)ï¼Œå¦‚123è¡¨ç¤º123Må‚æ•°\n",
    "    \n",
    "    è¿”å›:\n",
    "        å­—å…¸ï¼ŒåŒ…å«å„ç§æ•°æ®é‡ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. éœ€è¦çš„ token æ•°ï¼ˆå‚æ•°é‡ Ã— 20ï¼‰\n",
    "    tokens_billion = model_params_million / 1000 * 20\n",
    "    \n",
    "    # 2. éœ€è¦çš„å­—ç¬¦æ•°ï¼ˆ1 token â‰ˆ 4.8 å­—ç¬¦ï¼‰\n",
    "    chars_billion = tokens_billion * 4.8\n",
    "    \n",
    "    # 3. éœ€è¦çš„åˆ†ç‰‡æ•°ï¼ˆæ¯ä¸ªåˆ†ç‰‡ 250M å­—ç¬¦ï¼‰\n",
    "    num_shards = int(chars_billion * 1000 / 250)\n",
    "    \n",
    "    # 4. ç£ç›˜ç©ºé—´ï¼ˆæ¯ä¸ªåˆ†ç‰‡çº¦ 100MBï¼‰\n",
    "    disk_gb = num_shards * 100 / 1024\n",
    "    \n",
    "    return {\n",
    "        'model_params': f\"{model_params_million}M\",\n",
    "        'tokens': f\"{tokens_billion:.1f}B\",\n",
    "        'chars': f\"{chars_billion:.0f}B\",\n",
    "        'shards': num_shards,\n",
    "        'disk': f\"{disk_gb:.1f}GB\"\n",
    "    }\n",
    "\n",
    "# ä¸åŒè§„æ¨¡æ¨¡å‹\n",
    "models = {\n",
    "    'd10': 42,\n",
    "    'd12': 123,\n",
    "    'd20': 561,\n",
    "    'd26': 1200,\n",
    "    'd32': 2100\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, params in models.items():\n",
    "    req = calculate_data_requirement(params)\n",
    "    results.append({\n",
    "        'æ¨¡å‹': name,\n",
    "        'å‚æ•°é‡': req['model_params'],\n",
    "        'Tokenæ•°': req['tokens'],\n",
    "        'å­—ç¬¦æ•°': req['chars'],\n",
    "        'åˆ†ç‰‡æ•°': req['shards'],\n",
    "        'ç£ç›˜': req['disk']\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nğŸ“Š æ¨¡å‹æ•°æ®éœ€æ±‚è®¡ç®—è¡¨\\n\")\n",
    "display(df_results)\n",
    "print(\"\\nğŸ’¡ æç¤ºï¼šæ•°æ®é‡åŸºäº Chinchilla å®šå¾‹è®¡ç®—ï¼ˆå‚æ•°é‡ Ã— 20ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è‡ªå®šä¹‰è®¡ç®—\n",
    "\n",
    "è¾“å…¥ä½ çš„æ¨¡å‹å‚æ•°é‡ï¼Œè®¡ç®—éœ€è¦å¤šå°‘æ•°æ®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†è¯å™¨è®­ç»ƒä»£ç \n",
    "# å®Œæ•´ä»£ç è¯·æŸ¥çœ‹: scripts/tok_train.py\n",
    "\n",
    "print(\"ğŸ“„ åˆ†è¯å™¨è®­ç»ƒæ ¸å¿ƒä»£ç ï¼š\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# å‚æ•°è§£æ\n",
    "# å®Œæ•´ä»£ç ï¼šscripts/tok_train.py (ç¬¬ 16-23 è¡Œ)\n",
    "print(\"\"\"\n",
    "# è§£æå‘½ä»¤è¡Œå‚æ•°\n",
    "parser = argparse.ArgumentParser(description='Train a BPE tokenizer')\n",
    "parser.add_argument('--max_chars', type=int, default=10_000_000_000, \n",
    "                    help='æœ€å¤§è®­ç»ƒå­—ç¬¦æ•°ï¼ˆé»˜è®¤100äº¿ï¼‰')\n",
    "parser.add_argument('--doc_cap', type=int, default=10_000, \n",
    "                    help='æ¯ä¸ªæ–‡æ¡£çš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆé»˜è®¤10000ï¼‰')\n",
    "parser.add_argument('--vocab_size', type=int, default=65536, \n",
    "                    help='è¯æ±‡è¡¨å¤§å°ï¼ˆé»˜è®¤65536=2^16ï¼‰')\n",
    "args = parser.parse_args()\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# æ–‡æœ¬è¿­ä»£å™¨ï¼šä»æ•°æ®ä¸­è¯»å–æ–‡æœ¬\n",
    "# å®Œæ•´ä»£ç ï¼šscripts/tok_train.py (ç¬¬ 28-44 è¡Œ)\n",
    "print(\"\"\"\n",
    "# æ–‡æœ¬è¿­ä»£å™¨ï¼šä»æ•°æ®ä¸­è¯»å–æ–‡æœ¬\n",
    "def text_iterator():\n",
    "    \\\"\\\"\\\"ä»è®­ç»ƒæ•°æ®ä¸­è¿­ä»£è¯»å–æ–‡æœ¬\\\"\\\"\\\"\n",
    "    nchars = 0\n",
    "    for batch in parquets_iter_batched(split=\"train\"):\n",
    "        for doc in batch:\n",
    "            # é™åˆ¶æ¯ä¸ªæ–‡æ¡£çš„æœ€å¤§é•¿åº¦\n",
    "            doc_text = doc\n",
    "            if len(doc_text) > args.doc_cap:\n",
    "                doc_text = doc_text[:args.doc_cap]\n",
    "            nchars += len(doc_text)\n",
    "            yield doc_text\n",
    "            # è¾¾åˆ°æœ€å¤§å­—ç¬¦æ•°ååœæ­¢\n",
    "            if nchars > args.max_chars:\n",
    "                return\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# è®­ç»ƒåˆ†è¯å™¨\n",
    "# å®Œæ•´ä»£ç ï¼šscripts/tok_train.py (ç¬¬ 48-58 è¡Œ)\n",
    "print(\"\"\"\n",
    "# è®­ç»ƒåˆ†è¯å™¨\n",
    "text_iter = text_iterator()\n",
    "tokenizer = RustBPETokenizer.train_from_iterator(text_iter, args.vocab_size)\n",
    "\n",
    "# ä¿å­˜åˆ†è¯å™¨\n",
    "tokenizer_dir = os.path.join(base_dir, \"tokenizer\")\n",
    "tokenizer.save(tokenizer_dir)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nğŸ’¡ è¿è¡Œå‘½ä»¤: python -m scripts.tok_train --max_chars=2000000000\")\n",
    "print(\"   è¿™ä¼šä½¿ç”¨å‰ 20 äº¿å­—ç¬¦è®­ç»ƒä¸€ä¸ª 65536 è¯æ±‡çš„ BPE åˆ†è¯å™¨\")\n",
    "print(\"   å®Œæ•´ä»£ç è¯·æŸ¥çœ‹: scripts/tok_train.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. æ•°æ®åŠ è½½å™¨ä»£ç  (`nanochat/dataloader.py`)\n",
    "\n",
    "æ•°æ®åŠ è½½å™¨è´Ÿè´£å°†æ–‡æœ¬è½¬æ¢ä¸º token åºåˆ—ï¼Œå¹¶æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒï¼š\n",
    "\n",
    "**å…³é”®ä»£ç ä½ç½®ï¼š**\n",
    "- å®Œæ•´å®ç°ï¼š```nanochat/dataloader.py```\n",
    "- æ ¸å¿ƒç‰¹æ€§ï¼šå³æ—¶åˆ†è¯ã€æµå¼åŠ è½½ã€åˆ†å¸ƒå¼æ”¯æŒã€å¤šçº¿ç¨‹åˆ†è¯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®åŠ è½½å™¨æ ¸å¿ƒé€»è¾‘\n",
    "# å®Œæ•´ä»£ç è¯·æŸ¥çœ‹: nanochat/dataloader.py\n",
    "\n",
    "print(\"ğŸ“„ æ•°æ®åŠ è½½å™¨æ ¸å¿ƒé€»è¾‘ï¼š\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# æ•°æ®åŠ è½½å™¨å‡½æ•°ç­¾åå’Œæ ¸å¿ƒé€»è¾‘\n",
    "# å®Œæ•´ä»£ç ï¼šnanochat/dataloader.py (ç¬¬ 9-49 è¡Œ)\n",
    "print(\"\"\"\n",
    "def tokenizing_distributed_data_loader(B, T, split, tokenizer_threads=4, tokenizer_batch_size=128):\n",
    "    \\\"\\\"\\\"å³æ—¶åˆ†è¯çš„æ•°æ®åŠ è½½å™¨ï¼ˆæ”¯æŒåˆ†å¸ƒå¼ï¼‰\n",
    "    \n",
    "    å‚æ•°:\n",
    "        B: æ¯ä¸ª GPU çš„æ‰¹æ¬¡å¤§å°\n",
    "        T: åºåˆ—é•¿åº¦ï¼ˆä¸Šä¸‹æ–‡çª—å£ï¼‰\n",
    "        split: \"train\" æˆ– \"val\"\n",
    "        tokenizer_threads: åˆ†è¯å™¨çº¿ç¨‹æ•°\n",
    "        tokenizer_batch_size: åˆ†è¯æ‰¹æ¬¡å¤§å°\n",
    "    \\\"\\\"\\\"\n",
    "    # è·å–åˆ†å¸ƒå¼è®­ç»ƒä¿¡æ¯\n",
    "    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()\n",
    "    needed_tokens = B * T + 1  # +1 æ˜¯å› ä¸ºè¿˜éœ€è¦ç›®æ ‡ token\n",
    "    \n",
    "    # è·å–åˆ†è¯å™¨å’Œ BOS token\n",
    "    tokenizer = get_tokenizer()\n",
    "    bos_token = tokenizer.get_bos_token_id()\n",
    "    \n",
    "    # ä½¿ç”¨ deque ä½œä¸º token ç¼“å†²åŒºï¼ˆæµå¼å¤„ç†ï¼‰\n",
    "    token_buffer = deque()\n",
    "    scratch = torch.empty(needed_tokens, dtype=torch.int64, pin_memory=True)\n",
    "    \n",
    "    # æ— é™è¿­ä»£å™¨ï¼šä¸æ–­ç”Ÿæˆæ–‡æ¡£æ‰¹æ¬¡\n",
    "    def document_batches():\n",
    "        while True:\n",
    "            # ä» parquet æ–‡ä»¶è¯»å–æ‰¹æ¬¡ï¼ˆæ”¯æŒåˆ†å¸ƒå¼ï¼šstart=ddp_rank, step=ddp_world_sizeï¼‰\n",
    "            for batch in parquets_iter_batched(split=split, start=ddp_rank, step=ddp_world_size):\n",
    "                # å°†æ‰¹æ¬¡åˆ‡åˆ†æˆæ›´å°çš„å—ä¾›åˆ†è¯å™¨å¤„ç†\n",
    "                for i in range(0, len(batch), tokenizer_batch_size):\n",
    "                    yield batch[i:i+tokenizer_batch_size]\n",
    "    \n",
    "    batches = document_batches()\n",
    "    batch_index = 0\n",
    "    \n",
    "    while True:\n",
    "        # ç´¯ç§¯è¶³å¤Ÿçš„ token ç”¨äºä¸€æ¬¡è¿­ä»£\n",
    "        while len(token_buffer) < needed_tokens:\n",
    "            doc_batch = next(batches)\n",
    "            # æ‰¹é‡åˆ†è¯ï¼ˆå¤šçº¿ç¨‹åŠ é€Ÿï¼‰\n",
    "            token_lists = tokenizer.encode(doc_batch, prepend=bos_token, num_threads=tokenizer_threads)\n",
    "            # å°†æ‰€æœ‰ token æ·»åŠ åˆ°ç¼“å†²åŒº\n",
    "            for tokens in token_lists:\n",
    "                token_buffer.extend(tokens)\n",
    "            batch_index += 1\n",
    "        \n",
    "        # ä»ç¼“å†²åŒºå–å‡ºéœ€è¦çš„ token æ•°é‡\n",
    "        for i in range(needed_tokens):\n",
    "            scratch[i] = token_buffer.popleft()\n",
    "        \n",
    "        # åˆ›å»ºè¾“å…¥å’Œç›®æ ‡ï¼ˆç›®æ ‡å‘å³åç§» 1 ä½ï¼‰\n",
    "        inputs_cpu = scratch[:-1].to(dtype=torch.int32)\n",
    "        targets_cpu = scratch[1:]\n",
    "        \n",
    "        # é‡å¡‘ä¸º 2D å¹¶å¼‚æ­¥ç§»åŠ¨åˆ° GPU\n",
    "        inputs = inputs_cpu.view(B, T).to(device=\"cuda\", dtype=torch.int32, non_blocking=True)\n",
    "        targets = targets_cpu.view(B, T).to(device=\"cuda\", dtype=torch.int64, non_blocking=True)\n",
    "        yield inputs, targets\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nğŸ’¡ å…³é”®ç‰¹æ€§ï¼š\")\n",
    "print(\"   â€¢ å³æ—¶åˆ†è¯ï¼šä¸éœ€è¦é¢„å…ˆåˆ†è¯ï¼ŒèŠ‚çœç£ç›˜ç©ºé—´\")\n",
    "print(\"   â€¢ æµå¼åŠ è½½ï¼šåªåŠ è½½å½“å‰éœ€è¦çš„æ•°æ®ï¼ŒèŠ‚çœå†…å­˜\")\n",
    "print(\"   â€¢ åˆ†å¸ƒå¼æ”¯æŒï¼šæ¯ä¸ª GPU è‡ªåŠ¨åˆ’åˆ†æ•°æ®ï¼ˆstart=rank, step=world_sizeï¼‰\")\n",
    "print(\"   â€¢ åºåˆ—æ‰“åŒ…ï¼šè¿ç»­æ‹¼æ¥æ–‡æ¡£ï¼Œæœ€å¤§åŒ– GPU åˆ©ç”¨ç‡\")\n",
    "print(\"   â€¢ å¤šçº¿ç¨‹åˆ†è¯ï¼štokenizer_threads å‚æ•°åŠ é€Ÿåˆ†è¯è¿‡ç¨‹\")\n",
    "print(\"   å®Œæ•´ä»£ç è¯·æŸ¥çœ‹: nanochat/dataloader.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. æ•°æ®æ£€æŸ¥å·¥å…·ä»£ç \n",
    "\n",
    "é¡¹ç›®æä¾›äº†å®Œæ•´çš„æ•°æ®æ£€æŸ¥å·¥å…·é›†ï¼Œä½äº `data_check/` ç›®å½•ï¼š\n",
    "\n",
    "**å…³é”®ä»£ç ä½ç½®ï¼š**\n",
    "- æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ï¼š```data_check/check_data.py```\n",
    "- é•¿åº¦åˆ†å¸ƒæ£€æŸ¥ï¼š```data_check/check_length_distribution.py```\n",
    "- è‡ªå®šä¹‰æ•°æ®è½¬æ¢ï¼š```data_check/convert_custom_data.py```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®æ£€æŸ¥å·¥å…·ä»£ç \n",
    "# å®Œæ•´ä»£ç è¯·æŸ¥çœ‹: data_check/*.py\n",
    "\n",
    "print(\"ğŸ“„ æ•°æ®æ£€æŸ¥å·¥å…·ä»£ç ï¼š\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§\n",
    "# å®Œæ•´ä»£ç ï¼šdata_check/check_data.py (ç¬¬ 14-90 è¡Œ)\n",
    "print(\"\"\"\n",
    "# 1. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§ (data_check/check_data.py)\n",
    "def check_data_integrity(data_dir=None):\n",
    "    \\\"\\\"\\\"æ£€æŸ¥æ‰€æœ‰ Parquet æ–‡ä»¶çš„å®Œæ•´æ€§\\\"\\\"\\\"\n",
    "    if data_dir is None:\n",
    "        data_dir = os.path.expanduser(\"~/.cache/nanochat/base_data\")\n",
    "    \n",
    "    files = sorted(glob.glob(f\"{data_dir}/*.parquet\"))\n",
    "    broken = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    for filepath in files:\n",
    "        try:\n",
    "            table = pq.read_table(filepath)\n",
    "            rows = len(table)\n",
    "            total_rows += rows\n",
    "            \n",
    "            if rows == 0:\n",
    "                broken.append((filepath, \"ç©ºæ–‡ä»¶\"))\n",
    "            else:\n",
    "                print(f\"âœ… {os.path.basename(filepath)}: {rows:,} æ¡\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {os.path.basename(filepath)}: æŸå\")\n",
    "            broken.append((filepath, str(e)))\n",
    "    \n",
    "    return len(broken) == 0\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# 2. æ£€æŸ¥é•¿åº¦åˆ†å¸ƒ\n",
    "# å®Œæ•´ä»£ç ï¼šdata_check/check_length_distribution.py (ç¬¬ 13-73 è¡Œ)\n",
    "print(\"\"\"\n",
    "# 2. æ£€æŸ¥é•¿åº¦åˆ†å¸ƒ (data_check/check_length_distribution.py)\n",
    "def check_length_distribution(data_path):\n",
    "    \\\"\\\"\\\"åˆ†ææ–‡æœ¬é•¿åº¦åˆ†å¸ƒ\\\"\\\"\\\"\n",
    "    table = pq.read_table(data_path)\n",
    "    texts = table['text'].to_pylist()\n",
    "    \n",
    "    # ç»Ÿè®¡é•¿åº¦\n",
    "    lengths = [len(text) for text in texts]\n",
    "    avg_length = sum(lengths) / len(lengths)\n",
    "    min_length = min(lengths)\n",
    "    max_length = max(lengths)\n",
    "    \n",
    "    # åˆ†æ¡¶ç»Ÿè®¡\n",
    "    buckets = {\n",
    "        \"< 50\": 0, \"50-100\": 0, \"100-500\": 0, \n",
    "        \"500-1000\": 0, \"1000-2000\": 0, \"2000-5000\": 0, \"> 5000\": 0\n",
    "    }\n",
    "    for length in lengths:\n",
    "        if length < 50:\n",
    "            buckets[\"< 50\"] += 1\n",
    "        elif length < 100:\n",
    "            buckets[\"50-100\"] += 1\n",
    "        # ... å…¶ä»–åˆ†æ¡¶é€»è¾‘\n",
    "    \n",
    "    return buckets\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# 3. è½¬æ¢è‡ªå®šä¹‰æ•°æ®\n",
    "# å®Œæ•´ä»£ç ï¼šdata_check/convert_custom_data.py (ç¬¬ 42-69 è¡Œ)\n",
    "print(\"\"\"\n",
    "# 3. è½¬æ¢è‡ªå®šä¹‰æ•°æ® (data_check/convert_custom_data.py)\n",
    "def save_to_parquet(texts, output_dir, shard_size=100):\n",
    "    \\\"\\\"\\\"å°†æ–‡æœ¬åˆ—è¡¨è½¬æ¢ä¸º Parquet æ ¼å¼\\\"\\\"\\\"\n",
    "    import pyarrow as pa\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    num_shards = (len(texts) + shard_size - 1) // shard_size\n",
    "    \n",
    "    for i in range(num_shards):\n",
    "        start = i * shard_size\n",
    "        end = min(start + shard_size, len(texts))\n",
    "        shard_texts = texts[start:end]\n",
    "        \n",
    "        # åˆ›å»ºè¡¨æ ¼\n",
    "        table = pa.Table.from_pydict({'text': shard_texts})\n",
    "        output_path = os.path.join(output_dir, f\"shard_{i:05d}.parquet\")\n",
    "        \n",
    "        # ä¿å­˜ä¸º Parquetï¼ˆä½¿ç”¨ zstd å‹ç¼©ï¼‰\n",
    "        pq.write_table(\n",
    "            table,\n",
    "            output_path,\n",
    "            row_group_size=1024,\n",
    "            compression='zstd',\n",
    "            compression_level=3\n",
    "        )\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nğŸ’¡ ä½¿ç”¨æ–¹æ³•ï¼š\")\n",
    "print(\"   â€¢ python -m data_check.check_data                    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§\")\n",
    "print(\"   â€¢ python -m data_check.check_length_distribution    # æ£€æŸ¥é•¿åº¦åˆ†å¸ƒ\")\n",
    "print(\"   â€¢ python -m data_check.convert_custom_data          # è½¬æ¢è‡ªå®šä¹‰æ•°æ®\")\n",
    "print(\"   å®Œæ•´ä»£ç è¯·æŸ¥çœ‹: data_check/ ç›®å½•ä¸‹çš„å„ä¸ªæ–‡ä»¶\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. è®­ç»ƒè„šæœ¬å…³é”®å‚æ•° (`scripts/base_train.py`)\n",
    "\n",
    "é¢„è®­ç»ƒè„šæœ¬çš„å…³é”®é…ç½®å’Œä»£ç é€»è¾‘ï¼š\n",
    "\n",
    "**å…³é”®ä»£ç ä½ç½®ï¼š**\n",
    "- ç”¨æˆ·é…ç½®ï¼š```scripts/base_train.py```\n",
    "- è®­ç»ƒå¾ªç¯ï¼š```scripts/base_train.py```\n",
    "- ä¼˜åŒ–å™¨è®¾ç½®ï¼š```scripts/base_train.py```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒè„šæœ¬å…³é”®é…ç½®å’Œä»£ç \n",
    "# å®Œæ•´ä»£ç è¯·æŸ¥çœ‹: scripts/base_train.py\n",
    "\n",
    "print(\"ğŸ“„ é¢„è®­ç»ƒè„šæœ¬å…³é”®é…ç½®ï¼š\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ç”¨æˆ·é…ç½®\n",
    "# å®Œæ•´ä»£ç ï¼šscripts/base_train.py (ç¬¬ 28-56 è¡Œ)\n",
    "print(\"\"\"\n",
    "# ç”¨æˆ·é…ç½®\n",
    "depth = 20                    # Transformer æ·±åº¦\n",
    "max_seq_len = 2048            # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦\n",
    "device_batch_size = 32        # æ¯ä¸ª GPU çš„æ‰¹æ¬¡å¤§å°\n",
    "total_batch_size = 524288     # æ€»æ‰¹æ¬¡å¤§å°ï¼ˆtoken æ•°ï¼‰\n",
    "\n",
    "# è®­ç»ƒé•¿åº¦ï¼ˆä¸‰é€‰ä¸€ï¼ŒæŒ‰ä¼˜å…ˆçº§ï¼‰\n",
    "num_iterations = -1           # æ˜ç¡®çš„è¿­ä»£æ¬¡æ•°ï¼ˆ-1 = ç¦ç”¨ï¼‰\n",
    "target_flops = -1.0           # ç›®æ ‡ FLOPsï¼ˆ-1 = ç¦ç”¨ï¼‰\n",
    "target_param_data_ratio = 20 # Chinchilla å®šå¾‹ï¼šæ•°æ®tokenæ•° = å‚æ•°é‡ Ã— 20\n",
    "\n",
    "# ä¼˜åŒ–å™¨é…ç½®\n",
    "embedding_lr = 0.2            # åµŒå…¥å±‚å­¦ä¹ ç‡ï¼ˆAdamï¼‰\n",
    "unembedding_lr = 0.004        # è¾“å‡ºå±‚å­¦ä¹ ç‡ï¼ˆAdamï¼‰\n",
    "matrix_lr = 0.02              # çŸ©é˜µå‚æ•°å­¦ä¹ ç‡ï¼ˆMuonï¼‰\n",
    "grad_clip = 1.0               # æ¢¯åº¦è£å‰ªå€¼ï¼ˆ0.0 = ç¦ç”¨ï¼‰\n",
    "\n",
    "# è¯„ä¼°é…ç½®\n",
    "eval_every = 250              # æ¯ 250 æ­¥è¯„ä¼°ä¸€æ¬¡éªŒè¯é›† loss\n",
    "core_metric_every = 2000      # æ¯ 2000 æ­¥è¯„ä¼°ä¸€æ¬¡ CORE æŒ‡æ ‡\n",
    "sample_every = 2000           # æ¯ 2000 æ­¥é‡‡æ ·ä¸€æ¬¡\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# è®­ç»ƒå¾ªç¯æ ¸å¿ƒé€»è¾‘\n",
    "# å®Œæ•´ä»£ç ï¼šscripts/base_train.py (ç¬¬ 172-304 è¡Œ)\n",
    "print(\"\"\"\n",
    "# è®­ç»ƒå¾ªç¯æ ¸å¿ƒé€»è¾‘\n",
    "for step in range(num_iterations + 1):\n",
    "    # 1. è¯„ä¼°éªŒè¯é›† lossï¼ˆå®šæœŸï¼‰\n",
    "    if step % eval_every == 0:\n",
    "        model.eval()\n",
    "        val_bpb = evaluate_bpb(model, val_loader, eval_steps, token_bytes)\n",
    "        model.train()\n",
    "    \n",
    "    # 2. å•æ¬¡è®­ç»ƒæ­¥éª¤\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        # å‰å‘ä¼ æ’­\n",
    "        with autocast_ctx:\n",
    "            loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps  # æ¢¯åº¦ç´¯ç§¯å½’ä¸€åŒ–\n",
    "        \n",
    "        # åå‘ä¼ æ’­\n",
    "        loss.backward()\n",
    "        \n",
    "        # é¢„å–ä¸‹ä¸€æ‰¹æ•°æ®ï¼ˆå¼‚æ­¥ï¼‰\n",
    "        x, y = next(train_loader)\n",
    "    \n",
    "    # 3. æ¢¯åº¦è£å‰ª\n",
    "    if grad_clip > 0.0:\n",
    "        torch.nn.utils.clip_grad_norm_(orig_model.parameters(), grad_clip)\n",
    "    \n",
    "    # 4. æ›´æ–°å­¦ä¹ ç‡\n",
    "    lrm = get_lr_multiplier(step)\n",
    "    for opt in optimizers:\n",
    "        for group in opt.param_groups:\n",
    "            group[\"lr\"] = group[\"initial_lr\"] * lrm\n",
    "    \n",
    "    # 5. ä¼˜åŒ–å™¨æ­¥è¿›\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    \n",
    "    # 6. æ¸…ç†æ¢¯åº¦\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # 7. ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆæœ€åä¸€æ­¥ï¼‰\n",
    "    if step == num_iterations:\n",
    "        save_checkpoint(checkpoint_dir, step, orig_model.state_dict(), ...)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nğŸ’¡ è¿è¡Œå‘½ä»¤: torchrun --standalone --nproc_per_node=8 -m scripts.base_train --depth=20\")\n",
    "print(\"   å…³é”®å‚æ•°è¯´æ˜ï¼š\")\n",
    "print(\"   â€¢ depth: æ¨¡å‹æ·±åº¦ï¼Œå†³å®šå‚æ•°é‡\")\n",
    "print(\"   â€¢ target_param_data_ratio: Chinchilla å®šå¾‹æ¯”ä¾‹ï¼ˆé»˜è®¤ 20ï¼‰\")\n",
    "print(\"   â€¢ device_batch_size: æ¯ä¸ª GPU çš„æ‰¹æ¬¡å¤§å°\")\n",
    "print(\"   â€¢ total_batch_size: æ‰€æœ‰ GPU çš„æ€»æ‰¹æ¬¡å¤§å°ï¼ˆtoken æ•°ï¼‰\")\n",
    "print(\"   å®Œæ•´ä»£ç è¯·æŸ¥çœ‹: scripts/base_train.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. åˆ†è¯å™¨è¯„ä¼°ä»£ç  (`scripts/tok_eval.py`)\n",
    "\n",
    "è¯„ä¼°åˆ†è¯å™¨çš„å‹ç¼©ç‡ï¼Œå¹¶ä¸ GPT-2/GPT-4 åˆ†è¯å™¨å¯¹æ¯”ï¼š\n",
    "\n",
    "**å…³é”®ä»£ç ä½ç½®ï¼š**\n",
    "- è¯„ä¼°é€»è¾‘ï¼š```scripts/tok_eval.py```\n",
    "- æµ‹è¯•æ–‡æœ¬ï¼š```scripts/tok_eval.py```\n",
    "- å¯¹æ¯”å‡½æ•°ï¼š```scripts/tok_eval.py```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†è¯å™¨è¯„ä¼°ä»£ç \n",
    "# å®Œæ•´ä»£ç è¯·æŸ¥çœ‹: scripts/tok_eval.py\n",
    "\n",
    "print(\"ğŸ“„ åˆ†è¯å™¨è¯„ä¼°ä»£ç ï¼š\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# è¯„ä¼°åˆ†è¯å™¨å‹ç¼©ç‡çš„æ ¸å¿ƒé€»è¾‘\n",
    "# å®Œæ•´ä»£ç ï¼šscripts/tok_eval.py (ç¬¬ 152-190 è¡Œ)\n",
    "print(\"\"\"\n",
    "# è¯„ä¼°åˆ†è¯å™¨çš„å‹ç¼©ç‡\n",
    "from nanochat.tokenizer import get_tokenizer, RustBPETokenizer\n",
    "\n",
    "# æµ‹è¯•æ–‡æœ¬ï¼ˆæ–°é—»ã€ä»£ç ã€æ•°å­¦ç­‰ä¸åŒç±»å‹ï¼‰\n",
    "# å®Œæ•´ä»£ç ä¸­åŒ…å«äº†æ›´å¤šç±»å‹çš„æµ‹è¯•æ–‡æœ¬\n",
    "test_texts = [\n",
    "    (\"news\", news_text),      # æ–°é—»æ–‡æœ¬\n",
    "    (\"code\", code_text),      # ä»£ç æ–‡æœ¬\n",
    "    (\"math\", math_text),      # æ•°å­¦å…¬å¼\n",
    "    (\"fwe-train\", train_text), # è®­ç»ƒæ•°æ®\n",
    "]\n",
    "\n",
    "# å¯¹æ¯”ä¸åŒåˆ†è¯å™¨\n",
    "tokenizers = {}\n",
    "tokenizers[\"gpt2\"] = RustBPETokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizers[\"gpt4\"] = RustBPETokenizer.from_pretrained(\"cl100k_base\")\n",
    "tokenizers[\"ours\"] = get_tokenizer()\n",
    "\n",
    "results = {}\n",
    "\n",
    "for tokenizer_name, tokenizer in tokenizers.items():\n",
    "    vocab_sizes[tokenizer_name] = tokenizer.get_vocab_size()\n",
    "    results[tokenizer_name] = {}\n",
    "    \n",
    "    for name, text in test_texts:\n",
    "        # ç¼–ç æ–‡æœ¬\n",
    "        encoded = tokenizer.encode(text)\n",
    "        decoded = tokenizer.decode(encoded)\n",
    "        assert decoded == text  # éªŒè¯å¯é€†æ€§\n",
    "        \n",
    "        # è®¡ç®—å‹ç¼©ç‡ï¼ˆå­—èŠ‚æ•° / tokenæ•°ï¼‰\n",
    "        encoded_bytes = len(text.encode('utf-8'))\n",
    "        ratio = encoded_bytes / len(encoded)\n",
    "        \n",
    "        results[tokenizer_name][name] = {\n",
    "            'bytes': encoded_bytes,\n",
    "            'tokens': len(encoded),\n",
    "            'ratio': ratio\n",
    "        }\n",
    "\n",
    "# æ‰“å°å¯¹æ¯”ç»“æœï¼ˆè¯¦ç»†è¡¨æ ¼ï¼‰\n",
    "print_comparison(\"GPT-2\", results['gpt2'], results['ours'], test_texts)\n",
    "print_comparison(\"GPT-4\", results['gpt4'], results['ours'], test_texts)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nğŸ’¡ è¿è¡Œå‘½ä»¤: python -m scripts.tok_eval\")\n",
    "print(\"   è¿™ä¼šè¯„ä¼°åˆ†è¯å™¨åœ¨ä¸åŒç±»å‹æ–‡æœ¬ä¸Šçš„å‹ç¼©ç‡\")\n",
    "print(\"   å¹¶ä¸ GPT-2ã€GPT-4 çš„åˆ†è¯å™¨è¿›è¡Œå¯¹æ¯”\")\n",
    "print(\"   å®Œæ•´ä»£ç è¯·æŸ¥çœ‹: scripts/tok_eval.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‡ªå®šä¹‰æ¨¡å‹å‚æ•°é‡ï¼ˆå•ä½ï¼šç™¾ä¸‡ï¼‰\n",
    "my_model_params = 100  # ä¿®æ”¹è¿™é‡Œï¼\n",
    "\n",
    "result = calculate_data_requirement(my_model_params)\n",
    "\n",
    "print(f\"\\nğŸ¯ æ‚¨çš„æ¨¡å‹ï¼ˆ{my_model_params}M å‚æ•°ï¼‰éœ€è¦ï¼š\\n\")\n",
    "print(f\"   Token æ•°é‡ï¼š{result['tokens']}\")\n",
    "print(f\"   å­—ç¬¦æ•°é‡ï¼š{result['chars']}\")\n",
    "print(f\"   æ•°æ®åˆ†ç‰‡ï¼š{result['shards']} ä¸ª\")\n",
    "print(f\"   ç£ç›˜ç©ºé—´ï¼š{result['disk']}\")\n",
    "print(\"\\nä¸‹è½½å‘½ä»¤ï¼š\")\n",
    "print(f\"   python -m nanochat.dataset -n {result['shards']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"æ£€æŸ¥æ¸…å•\"></a>8. å®Œæ•´æµç¨‹æ£€æŸ¥æ¸…å•\n",
    "\n",
    "å‡†å¤‡å¥½æ•°æ®äº†å—ï¼Ÿå¯¹ç…§è¿™ä¸ªæ¸…å•æ£€æŸ¥ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def check_data_readiness():\n",
    "    \"\"\"æ£€æŸ¥æ•°æ®å‡†å¤‡æƒ…å†µ\"\"\"\n",
    "    \n",
    "    print(\"\\nğŸ” æ•°æ®å‡†å¤‡çŠ¶æ€æ£€æŸ¥\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    checks = []\n",
    "    \n",
    "    # 1. æ£€æŸ¥é¢„è®­ç»ƒæ•°æ®\n",
    "    base_data_dir = Path.home() / \".cache\" / \"nanochat\" / \"base_data\"\n",
    "    if base_data_dir.exists():\n",
    "        parquet_files = list(base_data_dir.glob(\"*.parquet\"))\n",
    "        if len(parquet_files) >= 8:\n",
    "            checks.append((\"âœ…\", f\"é¢„è®­ç»ƒæ•°æ®ï¼šæ‰¾åˆ° {len(parquet_files)} ä¸ªåˆ†ç‰‡\"))\n",
    "        else:\n",
    "            checks.append((\"âš ï¸\", f\"é¢„è®­ç»ƒæ•°æ®ï¼šåªæœ‰ {len(parquet_files)} ä¸ªåˆ†ç‰‡ï¼ˆå»ºè®®è‡³å°‘ 8 ä¸ªï¼‰\"))\n",
    "    else:\n",
    "        checks.append((\"âŒ\", \"é¢„è®­ç»ƒæ•°æ®ï¼šæœªä¸‹è½½\"))\n",
    "    \n",
    "    # 2. æ£€æŸ¥åˆ†è¯å™¨\n",
    "    tokenizer_dir = Path.home() / \".cache\" / \"nanochat\" / \"tokenizer\"\n",
    "    if tokenizer_dir.exists() and list(tokenizer_dir.glob(\"*.model\")):\n",
    "        checks.append((\"âœ…\", \"åˆ†è¯å™¨ï¼šå·²è®­ç»ƒ\"))\n",
    "    else:\n",
    "        checks.append((\"âš ï¸\", \"åˆ†è¯å™¨ï¼šæœªè®­ç»ƒï¼ˆéœ€è¦è¿è¡Œ tok_trainï¼‰\"))\n",
    "    \n",
    "    # 3. æ£€æŸ¥ç£ç›˜ç©ºé—´\n",
    "    cache_dir = Path.home() / \".cache\"\n",
    "    if cache_dir.exists():\n",
    "        try:\n",
    "            stat = shutil.disk_usage(cache_dir)\n",
    "            free_gb = stat.free / (1024**3)\n",
    "            if free_gb > 30:\n",
    "                checks.append((\"âœ…\", f\"ç£ç›˜ç©ºé—´ï¼šå‰©ä½™ {free_gb:.1f} GB\"))\n",
    "            else:\n",
    "                checks.append((\"âš ï¸\", f\"ç£ç›˜ç©ºé—´ï¼šå‰©ä½™ {free_gb:.1f} GBï¼ˆå»ºè®®è‡³å°‘ 30GBï¼‰\"))\n",
    "        except:\n",
    "            checks.append((\"â„¹ï¸\", \"ç£ç›˜ç©ºé—´ï¼šæ— æ³•æ£€æµ‹\"))\n",
    "    \n",
    "    # 4. æ£€æŸ¥ç¯å¢ƒå˜é‡\n",
    "    if 'HF_ENDPOINT' in os.environ:\n",
    "        checks.append((\"âœ…\", f\"HuggingFace é•œåƒï¼š{os.environ['HF_ENDPOINT']}\"))\n",
    "    else:\n",
    "        checks.append((\"â„¹ï¸\", \"HuggingFace é•œåƒï¼šæœªè®¾ç½®ï¼ˆå›½å†…ç”¨æˆ·å»ºè®®è®¾ç½®ï¼‰\"))\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    for status, msg in checks:\n",
    "        print(f\"{status} {msg}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # æ€»ç»“\n",
    "    ready_count = sum(1 for s, _ in checks if s == \"âœ…\")\n",
    "    total_count = len(checks)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š å°±ç»ªçŠ¶æ€ï¼š{ready_count}/{total_count}\")\n",
    "    \n",
    "    if ready_count >= 2:  # è‡³å°‘æœ‰æ•°æ®å’Œç©ºé—´å°±ç®—åŸºæœ¬å°±ç»ª\n",
    "        print(\"\\nğŸ‰ æ•°æ®åŸºæœ¬å‡†å¤‡å®Œæˆï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼\")\n",
    "    else:\n",
    "        print(\"\\nğŸ’¡ è¿˜æœ‰ä¸€äº›å‡†å¤‡å·¥ä½œéœ€è¦å®Œæˆï¼Œè¯·æŸ¥çœ‹ä¸Šé¢çš„æç¤º\")\n",
    "\n",
    "# è¿è¡Œæ£€æŸ¥\n",
    "check_data_readiness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ ä¸‹ä¸€æ­¥\n",
    "\n",
    "æ•°æ®å‡†å¤‡å¥½äº†ï¼æ¥ä¸‹æ¥ï¼š\n",
    "\n",
    "### 1. è®­ç»ƒåˆ†è¯å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒåˆ†è¯å™¨\n",
    "# âš ï¸ è­¦å‘Šï¼šè¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼\n",
    "\n",
    "# !python -m scripts.tok_train --max_chars=2000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. å¼€å§‹é¢„è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼€å§‹é¢„è®­ç»ƒï¼ˆéœ€è¦ GPUï¼‰\n",
    "# âš ï¸ è­¦å‘Šï¼šè¿™éœ€è¦å¤§é‡æ—¶é—´å’Œè®¡ç®—èµ„æºï¼\n",
    "\n",
    "# !torchrun --standalone --nproc_per_node=8 -m scripts.base_train --depth=20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

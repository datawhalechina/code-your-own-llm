{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📦 NanoChat 数据处理指南\n",
    "\n",
    "> **写给小白的话**：这个 Notebook 会手把手教你如何准备训练数据，不需要任何专业背景，跟着运行每个单元格就行！\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 目录\n",
    "\n",
    "1. [核心概念：3 分钟快速理解](#核心概念)\n",
    "2. [第一阶段：预训练数据](#预训练数据)\n",
    "3. [第二阶段：中期训练数据](#中期训练数据)\n",
    "4. [第三阶段：微调数据](#微调数据)\n",
    "5. [实战：准备中文数据](#准备中文数据)\n",
    "6. [数据质量检查工具](#数据质量检查)\n",
    "7. [数据量计算器](#数据量计算器)\n",
    "8. [完整流程检查清单](#检查清单)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"核心概念\"></a>1. 核心概念：3 分钟快速理解\n",
    "\n",
    "### 训练 AI 需要什么数据？\n",
    "\n",
    "想象一下教小孩学说话的过程：\n",
    "\n",
    "```\n",
    "👶 第一阶段：听大量日常对话 → 学会基本语言能力\n",
    "👧 第二阶段：学习问答方式 → 懂得对话结构  \n",
    "👨 第三阶段：学习回答问题 → 能按要求回答\n",
    "```\n",
    "\n",
    "训练 AI 模型也是一样的 **三个阶段**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 创建训练阶段对比表\n",
    "training_stages = pd.DataFrame({\n",
    "    '阶段': ['1️⃣', '2️⃣', '3️⃣'],\n",
    "    '名称': ['预训练 (Pretraining)', '中期训练 (Midtraining)', '微调 (Fine-tuning)'],\n",
    "    '数据类型': ['海量网页文本', '对话记录', '指令对话对'],\n",
    "    '学什么': ['语言的基本规律、语法、词汇、常识', '对话的格式、一问一答的结构', '理解和执行指令、做个好助手'],\n",
    "    '数据量': ['超级大 (几十 GB)', '中等 (几百 MB)', '较小 (几十 MB)']\n",
    "})\n",
    "\n",
    "print(\"\\n🎯 AI 训练的三个阶段\\n\")\n",
    "display(training_stages)\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💡 为什么要分三个阶段？\n",
    "\n",
    "**类比：就像学英语**\n",
    "\n",
    "- **预训练** = 大量阅读英文书籍（学语法和词汇）\n",
    "- **中期训练** = 学习英语对话（学怎么交流）\n",
    "- **微调** = 学习回答面试问题（学特定任务）\n",
    "\n",
    "如果直接让 AI 学习回答问题而不先学语言，就像让完全不懂英语的人直接参加英语面试，肯定学不好！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"预训练数据\"></a>2. 第一阶段：预训练数据\n",
    "\n",
    "### 用什么数据？\n",
    "\n",
    "项目默认使用 **FineWeb-Edu** 数据集：\n",
    "\n",
    "- 📖 来源：**Datawhale/fineweb-edu-100b-shuffle**（ModelScope 平台）\n",
    "- 🔗 访问地址：[https://modelscope.cn/datasets/Datawhale/fineweb-edu-100b-shuffle](https://modelscope.cn/datasets/Datawhale/fineweb-edu-100b-shuffle)\n",
    "- 📊 规模：约 1000 亿个单词（是的，1000 亿！）\n",
    "- ✨ 质量：高质量网页内容，已经过混洗处理\n",
    "- 🎁 免费：完全开源，直接下载\n",
    "- 🚀 **国内优势**：从 ModelScope 下载，国内访问速度更快更稳定\n",
    "\n",
    "### 📊 我需要下载多少数据？\n",
    "\n",
    "取决于你要训练多大的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同模型规模的数据需求对比表\n",
    "data_requirements = pd.DataFrame({\n",
    "    '模型规模': ['d10 (迷你)', 'd12 (小)', 'd20 (默认)', 'd26 (大)', 'd32 (超大)'],\n",
    "    '参数量': ['42M', '123M', '561M', '1.2B', '2.1B'],\n",
    "    '需要下载': ['16 个分片', '48 个分片', '215 个分片', '460 个分片', '806 个分片'],\n",
    "    '磁盘空间': ['~2GB', '~5GB', '~21GB', '~45GB', '~79GB'],\n",
    "    '训练时间': ['30 分钟', '1-2 小时', '4 小时', '12 小时', '24 小时']\n",
    "})\n",
    "\n",
    "print(\"\\n📊 模型规模与数据需求对照表\\n\")\n",
    "display(data_requirements)\n",
    "print(\"\\n💡 新手建议：先用 d10 或 d12 练手，熟悉流程后再训练大模型！\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 如何下载？\n",
    "\n",
    "**一条命令搞定！** 运行下面的代码单元格："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载 8 个分片用于训练分词器（约 800MB）\n",
    "# 这是最小下载量，适合快速测试\n",
    "\n",
    "!python -m nanochat.dataset -n 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果要训练 d20 模型，需要下载更多数据\n",
    "# ⚠️ 警告：这会下载约 21GB 数据，需要较长时间！\n",
    "# 如果不需要，请不要运行这个单元格\n",
    "\n",
    "# !python -m nanochat.dataset -n 215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📁 数据下载到哪了？\n",
    "\n",
    "所有数据自动保存到 `~/.cache/nanochat/base_data/`\n",
    "\n",
    "让我们检查一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 获取数据目录\n",
    "data_dir = Path.home() / \".cache\" / \"nanochat\" / \"base_data\"\n",
    "\n",
    "print(f\"📁 数据目录: {data_dir}\\n\")\n",
    "\n",
    "if data_dir.exists():\n",
    "    # 统计已下载的文件\n",
    "    parquet_files = list(data_dir.glob(\"*.parquet\"))\n",
    "    \n",
    "    if parquet_files:\n",
    "        print(f\"✅ 找到 {len(parquet_files)} 个数据文件\")\n",
    "        \n",
    "        # 计算总大小\n",
    "        total_size = sum(f.stat().st_size for f in parquet_files)\n",
    "        print(f\"💽 总大小: {total_size / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # 显示前 5 个文件\n",
    "        print(\"\\n前 5 个文件:\")\n",
    "        for f in sorted(parquet_files)[:5]:\n",
    "            size_mb = f.stat().st_size / (1024**2)\n",
    "            print(f\"  📄 {f.name:25s} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(\"⚠️ 数据目录存在，但没有找到 .parquet 文件\")\n",
    "        print(\"   请先运行上面的下载命令！\")\n",
    "else:\n",
    "    print(\"⚠️ 数据目录不存在，请先下载数据！\")\n",
    "    print(f\"   运行: python -m nanochat.dataset -n 8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 查看数据内容\n",
    "\n",
    "让我们打开一个文件看看里面是什么："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# 读取第一个分片\n",
    "data_dir = Path.home() / \".cache\" / \"nanochat\" / \"base_data\"\n",
    "parquet_files = list(data_dir.glob(\"*.parquet\")) if data_dir.exists() else []\n",
    "\n",
    "if parquet_files:\n",
    "    first_file = sorted(parquet_files)[0]\n",
    "    print(f\"📖 正在读取: {first_file.name}\\n\")\n",
    "    \n",
    "    # 读取 Parquet 文件\n",
    "    table = pq.read_table(first_file)\n",
    "    \n",
    "    print(f\"📊 文件信息:\")\n",
    "    print(f\"   行数: {len(table):,}\")\n",
    "    print(f\"   列名: {table.column_names}\")\n",
    "    \n",
    "    # 显示前 3 条数据\n",
    "    print(\"\\n📝 前 3 条数据示例:\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i in range(min(3, len(table))):\n",
    "        text = table['text'][i].as_py()\n",
    "        # 只显示前 200 个字符\n",
    "        preview = text[:200] + \"...\" if len(text) > 200 else text\n",
    "        print(f\"\\n第 {i+1} 条 (长度: {len(text)} 字符)\")\n",
    "        print(\"-\" * 80)\n",
    "        print(preview)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "else:\n",
    "    print(\"⚠️ 找不到数据文件，请先下载数据！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💻 数据下载代码详解\n",
    "\n",
    "数据下载功能由 `nanochat/dataset.py` 实现，核心代码如下：\n",
    "\n",
    "**核心功能：**\n",
    "1. **多进程并行下载**：默认使用 4 个进程同时下载\n",
    "2. **自动重试机制**：下载失败时自动重试，最多 5 次\n",
    "3. **断点续传**：已下载的文件会自动跳过\n",
    "4. **临时文件保护**：先下载到临时文件，完成后才重命名，避免中断导致文件损坏\n",
    "\n",
    "**数据源配置：**\n",
    "- 默认使用 ModelScope：`Datawhale/fineweb-edu-100b-shuffle`\n",
    "- 国内访问速度快，无需特殊配置\n",
    "- 总共 1822 个分片，每个约 100MB\n",
    "\n",
    "**关键代码位置：**\n",
    "- 数据源配置：```nanochat/dataset.py```\n",
    "- 下载函数：```nanochat/dataset.py```\n",
    "- 主程序：```nanochat/dataset.py```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看 dataset.py 的关键代码\n",
    "# 完整代码在: nanochat/dataset.py\n",
    "\n",
    "print(\"📄 数据下载模块核心代码：\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 数据源配置（已优化为国内源）\n",
    "# 完整代码：nanochat/dataset.py (第 22-29 行)\n",
    "print(\"\"\"\n",
    "# 数据源配置（已优化为国内源）\n",
    "BASE_URL = \"https://modelscope.cn/api/v1/datasets/Datawhale/fineweb-edu-100b-shuffle/repo?Revision=master&FilePath=\"\n",
    "MAX_SHARD = 1822  # 总共 1822 个分片\n",
    "index_to_filename = lambda index: f\"shard_{index:05d}.parquet\"  # 文件名格式\n",
    "DATA_DIR = os.path.join(base_dir, \"base_data\")  # 数据保存目录\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# 下载单个文件的函数（带重试机制）\n",
    "# 完整代码：nanochat/dataset.py (第 61-110 行)\n",
    "print(\"\"\"\n",
    "def download_single_file(index):\n",
    "    \\\"\\\"\\\"下载单个文件，带重试机制\\\"\\\"\\\"\n",
    "    filename = index_to_filename(index)\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    \n",
    "    # 已存在则跳过（断点续传）\n",
    "    if os.path.exists(filepath):\n",
    "        return True\n",
    "    \n",
    "    url = f\"{BASE_URL}/{filename}\"\n",
    "    \n",
    "    # 最多重试 5 次，指数退避\n",
    "    max_attempts = 5\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        try:\n",
    "            response = requests.get(url, stream=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # 先写入临时文件（临时文件保护）\n",
    "            temp_path = filepath + \".tmp\"\n",
    "            with open(temp_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024*1024):  # 1MB chunks\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            \n",
    "            # 原子操作：重命名临时文件\n",
    "            os.rename(temp_path, filepath)\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            # 清理部分文件\n",
    "            if attempt < max_attempts:\n",
    "                wait_time = 2 ** attempt  # 指数退避\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                return False\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# 多进程并行下载\n",
    "# 完整代码：nanochat/dataset.py (第 113-129 行)\n",
    "print(\"\"\"\n",
    "# 使用多进程并行下载（默认 4 个进程）\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-n\", \"--num-files\", type=int, default=-1)\n",
    "parser.add_argument(\"-w\", \"--num-workers\", type=int, default=4)\n",
    "args = parser.parse_args()\n",
    "\n",
    "ids_to_download = list(range(num))\n",
    "with Pool(processes=args.num_workers) as pool:\n",
    "    results = pool.map(download_single_file, ids_to_download)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n💡 完整代码请查看: nanochat/dataset.py\")\n",
    "print(\"   关键特性：多进程、自动重试、断点续传、临时文件保护\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"中期训练数据\"></a>3. 第二阶段：中期训练数据\n",
    "\n",
    "### 用什么数据？\n",
    "\n",
    "项目默认使用 **SmolTalk** 对话数据集：\n",
    "\n",
    "#### 🌐 数据源信息\n",
    "\n",
    "- 📖 数据集：`HuggingFaceTB/smoltalk`\n",
    "- 🏢 平台：HuggingFace\n",
    "- 🗣️ 内容：真实的人类对话记录\n",
    "- 📝 格式：一问一答的对话形式\n",
    "- 🎯 目的：让模型学会对话的格式\n",
    "- 📥 下载方式：训练脚本自动下载\n",
    "\n",
    "#### 🇨🇳 国内访问优化\n",
    "\n",
    "如果下载速度慢，可以设置 HuggingFace 镜像加速：\n",
    "\n",
    "```bash\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "```\n",
    "\n",
    "### 数据格式示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 对话数据格式示例\n",
    "dialogue_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"你好！请介绍一下自己\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"你好！我是一个 AI 助手，可以回答问题、提供建议...\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"你会说中文吗？\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"是的，我可以使用中文交流。\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"📝 对话数据格式示例：\\n\")\n",
    "print(json.dumps(dialogue_example, ensure_ascii=False, indent=2))\n",
    "\n",
    "print(\"\\n💡 重要字段说明：\")\n",
    "print(\"   • role: 说话的角色，'user'(用户) 或 'assistant'(助手)\")\n",
    "print(\"   • content: 说话的内容\")\n",
    "\n",
    "print(\"\\n✅ 好消息：训练脚本会自动下载 SmolTalk 数据集，无需手动操作！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"微调数据\"></a>4. 第三阶段：微调数据\n",
    "\n",
    "### 用什么数据？\n",
    "\n",
    "微调阶段混合使用多个任务数据集：\n",
    "\n",
    "#### 🌐 数据集列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 微调数据集概览\n",
    "sft_datasets = pd.DataFrame({\n",
    "    '数据集': ['ARC-Easy', 'ARC-Challenge', 'GSM8K', 'SmolTalk'],\n",
    "    '内容': ['简单选择题', '困难选择题', '小学数学题', '日常对话'],\n",
    "    '数量': ['2,300 条', '1,100 条', '8,000 条', '10,000 条'],\n",
    "    '学什么能力': ['常识推理', '深度推理', '数学计算', '闲聊能力']\n",
    "})\n",
    "\n",
    "print(\"\\n🎯 微调阶段的数据集\\n\")\n",
    "display(sft_datasets)\n",
    "print(\"\\n📊 总计：约 21,400 条训练样本\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🇨🇳 国内访问优化\n",
    "\n",
    "所有微调数据集来自 HuggingFace，会在训练时自动下载。国内用户建议设置镜像：\n",
    "\n",
    "```bash\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "```\n",
    "\n",
    "或在 Python 代码中设置：\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "```\n",
    "\n",
    "### 数据格式示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数学题示例 (GSM8K)\n",
    "math_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"小明有8个苹果，吃掉了3个，还剩几个？\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"让我来算一下：\\n8 - 3 = 5\\n所以小明还剩5个苹果。\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 选择题示例 (ARC)\n",
    "arc_example = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"哪个物体会浮在水面上？\\nA. 石头\\nB. 铁钉\\nC. 木头\\nD. 玻璃球\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"答案是C. 木头。因为木头的密度比水小，所以会浮在水面上。\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"📝 数学题示例 (GSM8K)：\\n\")\n",
    "print(json.dumps(math_example, ensure_ascii=False, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"📝 选择题示例 (ARC)：\\n\")\n",
    "print(json.dumps(arc_example, ensure_ascii=False, indent=2))\n",
    "\n",
    "print(\"\\n✅ 这些数据集会在运行微调脚本时自动下载！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"准备中文数据\"></a>5. 实战：准备中文数据\n",
    "\n",
    "> 如果你想训练中文模型，需要准备中文数据。下面是一个完整的示例！\n",
    "\n",
    "### 方法一：使用 HuggingFace 中文数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置镜像（可选）\n",
    "import os\n",
    "\n",
    "# 项目已默认使用 ModelScope 下载预训练数据，无需额外设置\n",
    "# 以下镜像设置仅用于其他 HuggingFace 数据集（如 SmolTalk）\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "print(\"✅ 已设置 HuggingFace 镜像：https://hf-mirror.com\")\n",
    "print(\"   这会加速其他 HuggingFace 数据集的下载速度\")\n",
    "print(\"\\n💡 预训练数据默认从 ModelScope 下载，国内访问速度已优化\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载中文维基百科数据（示例）\n",
    "# ⚠️ 警告：这会下载较大的数据集，需要时间！\n",
    "# 如果不需要，请不要运行这个单元格\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"📥 正在下载中文维基百科（前 1000 条用于演示）...\\n\")\n",
    "\n",
    "try:\n",
    "    # 只下载前 1000 条用于演示\n",
    "    wiki = load_dataset(\n",
    "        \"wikipedia\",\n",
    "        \"20220301.zh\",  # 中文版本\n",
    "        split=\"train[:1000]\",  # 只取前 1000 条\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ 成功下载：{len(wiki):,} 条数据\\n\")\n",
    "    \n",
    "    # 显示第一条\n",
    "    print(\"📝 第一条数据示例：\")\n",
    "    print(\"=\"*80)\n",
    "    first_text = wiki[0]['text'][:300] + \"...\"\n",
    "    print(first_text)\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 下载失败：{e}\")\n",
    "    print(\"   可能需要检查网络连接或尝试使用镜像\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法二：转换自己的文本数据\n",
    "\n",
    "如果你有自己收集的中文文本，可以使用项目提供的转换工具："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用内置工具转换自定义数据\n",
    "# 详细说明请查看 data_check/convert_custom_data.py\n",
    "\n",
    "print(\"🛠️ 转换自定义文本数据的步骤：\\n\")\n",
    "print(\"1. 准备你的文本数据（.txt 文件）\")\n",
    "print(\"2. 运行转换命令：\")\n",
    "print(\"   python -m data_check.convert_custom_data\")\n",
    "print(\"\\n支持的输入格式：\")\n",
    "print(\"   • 单个文本文件（每行一条数据）\")\n",
    "print(\"   • 目录（包含多个 .txt 文件）\")\n",
    "print(\"\\n详细代码请查看：data_check/convert_custom_data.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"数据质量检查\"></a>6. 数据质量检查工具\n",
    "\n",
    "项目提供了完整的数据检查工具集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据检查工具概览\n",
    "tools = pd.DataFrame({\n",
    "    '工具': [\n",
    "        'check_data.py',\n",
    "        'check_length_distribution.py',\n",
    "        'check_content_quality.py',\n",
    "        'check_char_distribution.py',\n",
    "        'convert_custom_data.py'\n",
    "    ],\n",
    "    '用途': [\n",
    "        '验证数据文件完整性',\n",
    "        '检查文本长度分布',\n",
    "        '抽样检查内容质量',\n",
    "        '检查字符分布统计',\n",
    "        '转换自定义文本数据'\n",
    "    ],\n",
    "    '命令': [\n",
    "        'python -m data_check.check_data',\n",
    "        'python -m data_check.check_length_distribution',\n",
    "        'python -m data_check.check_content_quality',\n",
    "        'python -m data_check.check_char_distribution',\n",
    "        'python -m data_check.convert_custom_data'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n🛠️ 数据检查工具总览\\n\")\n",
    "display(tools)\n",
    "print(\"\\n💡 所有工具的详细代码都在 data_check/ 目录下\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 快速检查数据完整性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行数据完整性检查\n",
    "!python -m data_check.check_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检查文本长度分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析数据的长度分布\n",
    "# 这有助于了解数据质量\n",
    "\n",
    "!python -m data_check.check_length_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"数据量计算器\"></a>7. 数据量计算器\n",
    "\n",
    "### Chinchilla 定律\n",
    "\n",
    "**数据 token 数 = 模型参数量 × 20**\n",
    "\n",
    "让我们计算不同模型需要多少数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化数据量对比\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 设置中文字体（如果有的话）\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 准备数据\n",
    "models_list = [r['模型'] for r in results]\n",
    "params_list = [float(r['参数量'].replace('M', '')) for r in results]\n",
    "tokens_list = [float(r['Token数'].replace('B', '')) for r in results]\n",
    "disk_list = [float(r['磁盘'].replace('GB', '')) for r in results]\n",
    "shards_list = [r['分片数'] for r in results]\n",
    "\n",
    "# 创建多子图\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('📊 不同模型规模的数据需求对比', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. 参数量 vs Token数\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(params_list, tokens_list, 'o-', linewidth=2, markersize=8, color='#4CAF50')\n",
    "ax1.set_xlabel('模型参数量 (M)', fontsize=12)\n",
    "ax1.set_ylabel('需要的 Token 数 (B)', fontsize=12)\n",
    "ax1.set_title('参数量 vs Token数（Chinchilla 定律）', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "for i, model in enumerate(models_list):\n",
    "    ax1.annotate(model, (params_list[i], tokens_list[i]), \n",
    "                textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=10)\n",
    "\n",
    "# 2. Token数 vs 磁盘空间\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(tokens_list, disk_list, 's-', linewidth=2, markersize=8, color='#2196F3')\n",
    "ax2.set_xlabel('Token 数 (B)', fontsize=12)\n",
    "ax2.set_ylabel('磁盘空间 (GB)', fontsize=12)\n",
    "ax2.set_title('Token数 vs 磁盘空间', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "for i, model in enumerate(models_list):\n",
    "    ax2.annotate(model, (tokens_list[i], disk_list[i]), \n",
    "                textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=10)\n",
    "\n",
    "# 3. 分片数对比（柱状图）\n",
    "ax3 = axes[1, 0]\n",
    "bars = ax3.bar(models_list, shards_list, color=['#FF9800', '#F44336', '#9C27B0', '#00BCD4', '#4CAF50'], alpha=0.7)\n",
    "ax3.set_xlabel('模型规模', fontsize=12)\n",
    "ax3.set_ylabel('分片数', fontsize=12)\n",
    "ax3.set_title('不同模型需要的分片数', fontsize=13, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "# 在柱状图上添加数值标签\n",
    "for bar, shard in zip(bars, shards_list):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{shard}',\n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 4. 磁盘空间对比（柱状图）\n",
    "ax4 = axes[1, 1]\n",
    "bars2 = ax4.bar(models_list, disk_list, color=['#FF9800', '#F44336', '#9C27B0', '#00BCD4', '#4CAF50'], alpha=0.7)\n",
    "ax4.set_xlabel('模型规模', fontsize=12)\n",
    "ax4.set_ylabel('磁盘空间 (GB)', fontsize=12)\n",
    "ax4.set_title('不同模型需要的磁盘空间', fontsize=13, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "# 在柱状图上添加数值标签\n",
    "for bar, disk in zip(bars2, disk_list):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{disk:.1f}GB',\n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ 可视化图表已生成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_data_requirement(model_params_million):\n",
    "    \"\"\"\n",
    "    计算训练所需的数据量\n",
    "    \n",
    "    参数:\n",
    "        model_params_million: 模型参数量(百万)，如123表示123M参数\n",
    "    \n",
    "    返回:\n",
    "        字典，包含各种数据量信息\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 需要的 token 数（参数量 × 20）\n",
    "    tokens_billion = model_params_million / 1000 * 20\n",
    "    \n",
    "    # 2. 需要的字符数（1 token ≈ 4.8 字符）\n",
    "    chars_billion = tokens_billion * 4.8\n",
    "    \n",
    "    # 3. 需要的分片数（每个分片 250M 字符）\n",
    "    num_shards = int(chars_billion * 1000 / 250)\n",
    "    \n",
    "    # 4. 磁盘空间（每个分片约 100MB）\n",
    "    disk_gb = num_shards * 100 / 1024\n",
    "    \n",
    "    return {\n",
    "        'model_params': f\"{model_params_million}M\",\n",
    "        'tokens': f\"{tokens_billion:.1f}B\",\n",
    "        'chars': f\"{chars_billion:.0f}B\",\n",
    "        'shards': num_shards,\n",
    "        'disk': f\"{disk_gb:.1f}GB\"\n",
    "    }\n",
    "\n",
    "# 不同规模模型\n",
    "models = {\n",
    "    'd10': 42,\n",
    "    'd12': 123,\n",
    "    'd20': 561,\n",
    "    'd26': 1200,\n",
    "    'd32': 2100\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, params in models.items():\n",
    "    req = calculate_data_requirement(params)\n",
    "    results.append({\n",
    "        '模型': name,\n",
    "        '参数量': req['model_params'],\n",
    "        'Token数': req['tokens'],\n",
    "        '字符数': req['chars'],\n",
    "        '分片数': req['shards'],\n",
    "        '磁盘': req['disk']\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n📊 模型数据需求计算表\\n\")\n",
    "display(df_results)\n",
    "print(\"\\n💡 提示：数据量基于 Chinchilla 定律计算（参数量 × 20）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义计算\n",
    "\n",
    "输入你的模型参数量，计算需要多少数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词器训练代码\n",
    "# 完整代码请查看: scripts/tok_train.py\n",
    "\n",
    "print(\"📄 分词器训练核心代码：\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 参数解析\n",
    "# 完整代码：scripts/tok_train.py (第 16-23 行)\n",
    "print(\"\"\"\n",
    "# 解析命令行参数\n",
    "parser = argparse.ArgumentParser(description='Train a BPE tokenizer')\n",
    "parser.add_argument('--max_chars', type=int, default=10_000_000_000, \n",
    "                    help='最大训练字符数（默认100亿）')\n",
    "parser.add_argument('--doc_cap', type=int, default=10_000, \n",
    "                    help='每个文档的最大字符数（默认10000）')\n",
    "parser.add_argument('--vocab_size', type=int, default=65536, \n",
    "                    help='词汇表大小（默认65536=2^16）')\n",
    "args = parser.parse_args()\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# 文本迭代器：从数据中读取文本\n",
    "# 完整代码：scripts/tok_train.py (第 28-44 行)\n",
    "print(\"\"\"\n",
    "# 文本迭代器：从数据中读取文本\n",
    "def text_iterator():\n",
    "    \\\"\\\"\\\"从训练数据中迭代读取文本\\\"\\\"\\\"\n",
    "    nchars = 0\n",
    "    for batch in parquets_iter_batched(split=\"train\"):\n",
    "        for doc in batch:\n",
    "            # 限制每个文档的最大长度\n",
    "            doc_text = doc\n",
    "            if len(doc_text) > args.doc_cap:\n",
    "                doc_text = doc_text[:args.doc_cap]\n",
    "            nchars += len(doc_text)\n",
    "            yield doc_text\n",
    "            # 达到最大字符数后停止\n",
    "            if nchars > args.max_chars:\n",
    "                return\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# 训练分词器\n",
    "# 完整代码：scripts/tok_train.py (第 48-58 行)\n",
    "print(\"\"\"\n",
    "# 训练分词器\n",
    "text_iter = text_iterator()\n",
    "tokenizer = RustBPETokenizer.train_from_iterator(text_iter, args.vocab_size)\n",
    "\n",
    "# 保存分词器\n",
    "tokenizer_dir = os.path.join(base_dir, \"tokenizer\")\n",
    "tokenizer.save(tokenizer_dir)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n💡 运行命令: python -m scripts.tok_train --max_chars=2000000000\")\n",
    "print(\"   这会使用前 20 亿字符训练一个 65536 词汇的 BPE 分词器\")\n",
    "print(\"   完整代码请查看: scripts/tok_train.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 数据加载器代码 (`nanochat/dataloader.py`)\n",
    "\n",
    "数据加载器负责将文本转换为 token 序列，并支持分布式训练：\n",
    "\n",
    "**关键代码位置：**\n",
    "- 完整实现：```nanochat/dataloader.py```\n",
    "- 核心特性：即时分词、流式加载、分布式支持、多线程分词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载器核心逻辑\n",
    "# 完整代码请查看: nanochat/dataloader.py\n",
    "\n",
    "print(\"📄 数据加载器核心逻辑：\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 数据加载器函数签名和核心逻辑\n",
    "# 完整代码：nanochat/dataloader.py (第 9-49 行)\n",
    "print(\"\"\"\n",
    "def tokenizing_distributed_data_loader(B, T, split, tokenizer_threads=4, tokenizer_batch_size=128):\n",
    "    \\\"\\\"\\\"即时分词的数据加载器（支持分布式）\n",
    "    \n",
    "    参数:\n",
    "        B: 每个 GPU 的批次大小\n",
    "        T: 序列长度（上下文窗口）\n",
    "        split: \"train\" 或 \"val\"\n",
    "        tokenizer_threads: 分词器线程数\n",
    "        tokenizer_batch_size: 分词批次大小\n",
    "    \\\"\\\"\\\"\n",
    "    # 获取分布式训练信息\n",
    "    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()\n",
    "    needed_tokens = B * T + 1  # +1 是因为还需要目标 token\n",
    "    \n",
    "    # 获取分词器和 BOS token\n",
    "    tokenizer = get_tokenizer()\n",
    "    bos_token = tokenizer.get_bos_token_id()\n",
    "    \n",
    "    # 使用 deque 作为 token 缓冲区（流式处理）\n",
    "    token_buffer = deque()\n",
    "    scratch = torch.empty(needed_tokens, dtype=torch.int64, pin_memory=True)\n",
    "    \n",
    "    # 无限迭代器：不断生成文档批次\n",
    "    def document_batches():\n",
    "        while True:\n",
    "            # 从 parquet 文件读取批次（支持分布式：start=ddp_rank, step=ddp_world_size）\n",
    "            for batch in parquets_iter_batched(split=split, start=ddp_rank, step=ddp_world_size):\n",
    "                # 将批次切分成更小的块供分词器处理\n",
    "                for i in range(0, len(batch), tokenizer_batch_size):\n",
    "                    yield batch[i:i+tokenizer_batch_size]\n",
    "    \n",
    "    batches = document_batches()\n",
    "    batch_index = 0\n",
    "    \n",
    "    while True:\n",
    "        # 累积足够的 token 用于一次迭代\n",
    "        while len(token_buffer) < needed_tokens:\n",
    "            doc_batch = next(batches)\n",
    "            # 批量分词（多线程加速）\n",
    "            token_lists = tokenizer.encode(doc_batch, prepend=bos_token, num_threads=tokenizer_threads)\n",
    "            # 将所有 token 添加到缓冲区\n",
    "            for tokens in token_lists:\n",
    "                token_buffer.extend(tokens)\n",
    "            batch_index += 1\n",
    "        \n",
    "        # 从缓冲区取出需要的 token 数量\n",
    "        for i in range(needed_tokens):\n",
    "            scratch[i] = token_buffer.popleft()\n",
    "        \n",
    "        # 创建输入和目标（目标向右偏移 1 位）\n",
    "        inputs_cpu = scratch[:-1].to(dtype=torch.int32)\n",
    "        targets_cpu = scratch[1:]\n",
    "        \n",
    "        # 重塑为 2D 并异步移动到 GPU\n",
    "        inputs = inputs_cpu.view(B, T).to(device=\"cuda\", dtype=torch.int32, non_blocking=True)\n",
    "        targets = targets_cpu.view(B, T).to(device=\"cuda\", dtype=torch.int64, non_blocking=True)\n",
    "        yield inputs, targets\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n💡 关键特性：\")\n",
    "print(\"   • 即时分词：不需要预先分词，节省磁盘空间\")\n",
    "print(\"   • 流式加载：只加载当前需要的数据，节省内存\")\n",
    "print(\"   • 分布式支持：每个 GPU 自动划分数据（start=rank, step=world_size）\")\n",
    "print(\"   • 序列打包：连续拼接文档，最大化 GPU 利用率\")\n",
    "print(\"   • 多线程分词：tokenizer_threads 参数加速分词过程\")\n",
    "print(\"   完整代码请查看: nanochat/dataloader.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 数据检查工具代码\n",
    "\n",
    "项目提供了完整的数据检查工具集，位于 `data_check/` 目录：\n",
    "\n",
    "**关键代码位置：**\n",
    "- 数据完整性检查：```data_check/check_data.py```\n",
    "- 长度分布检查：```data_check/check_length_distribution.py```\n",
    "- 自定义数据转换：```data_check/convert_custom_data.py```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据检查工具代码\n",
    "# 完整代码请查看: data_check/*.py\n",
    "\n",
    "print(\"📄 数据检查工具代码：\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. 检查数据完整性\n",
    "# 完整代码：data_check/check_data.py (第 14-90 行)\n",
    "print(\"\"\"\n",
    "# 1. 检查数据完整性 (data_check/check_data.py)\n",
    "def check_data_integrity(data_dir=None):\n",
    "    \\\"\\\"\\\"检查所有 Parquet 文件的完整性\\\"\\\"\\\"\n",
    "    if data_dir is None:\n",
    "        data_dir = os.path.expanduser(\"~/.cache/nanochat/base_data\")\n",
    "    \n",
    "    files = sorted(glob.glob(f\"{data_dir}/*.parquet\"))\n",
    "    broken = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    for filepath in files:\n",
    "        try:\n",
    "            table = pq.read_table(filepath)\n",
    "            rows = len(table)\n",
    "            total_rows += rows\n",
    "            \n",
    "            if rows == 0:\n",
    "                broken.append((filepath, \"空文件\"))\n",
    "            else:\n",
    "                print(f\"✅ {os.path.basename(filepath)}: {rows:,} 条\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {os.path.basename(filepath)}: 损坏\")\n",
    "            broken.append((filepath, str(e)))\n",
    "    \n",
    "    return len(broken) == 0\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# 2. 检查长度分布\n",
    "# 完整代码：data_check/check_length_distribution.py (第 13-73 行)\n",
    "print(\"\"\"\n",
    "# 2. 检查长度分布 (data_check/check_length_distribution.py)\n",
    "def check_length_distribution(data_path):\n",
    "    \\\"\\\"\\\"分析文本长度分布\\\"\\\"\\\"\n",
    "    table = pq.read_table(data_path)\n",
    "    texts = table['text'].to_pylist()\n",
    "    \n",
    "    # 统计长度\n",
    "    lengths = [len(text) for text in texts]\n",
    "    avg_length = sum(lengths) / len(lengths)\n",
    "    min_length = min(lengths)\n",
    "    max_length = max(lengths)\n",
    "    \n",
    "    # 分桶统计\n",
    "    buckets = {\n",
    "        \"< 50\": 0, \"50-100\": 0, \"100-500\": 0, \n",
    "        \"500-1000\": 0, \"1000-2000\": 0, \"2000-5000\": 0, \"> 5000\": 0\n",
    "    }\n",
    "    for length in lengths:\n",
    "        if length < 50:\n",
    "            buckets[\"< 50\"] += 1\n",
    "        elif length < 100:\n",
    "            buckets[\"50-100\"] += 1\n",
    "        # ... 其他分桶逻辑\n",
    "    \n",
    "    return buckets\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# 3. 转换自定义数据\n",
    "# 完整代码：data_check/convert_custom_data.py (第 42-69 行)\n",
    "print(\"\"\"\n",
    "# 3. 转换自定义数据 (data_check/convert_custom_data.py)\n",
    "def save_to_parquet(texts, output_dir, shard_size=100):\n",
    "    \\\"\\\"\\\"将文本列表转换为 Parquet 格式\\\"\\\"\\\"\n",
    "    import pyarrow as pa\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    num_shards = (len(texts) + shard_size - 1) // shard_size\n",
    "    \n",
    "    for i in range(num_shards):\n",
    "        start = i * shard_size\n",
    "        end = min(start + shard_size, len(texts))\n",
    "        shard_texts = texts[start:end]\n",
    "        \n",
    "        # 创建表格\n",
    "        table = pa.Table.from_pydict({'text': shard_texts})\n",
    "        output_path = os.path.join(output_dir, f\"shard_{i:05d}.parquet\")\n",
    "        \n",
    "        # 保存为 Parquet（使用 zstd 压缩）\n",
    "        pq.write_table(\n",
    "            table,\n",
    "            output_path,\n",
    "            row_group_size=1024,\n",
    "            compression='zstd',\n",
    "            compression_level=3\n",
    "        )\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n💡 使用方法：\")\n",
    "print(\"   • python -m data_check.check_data                    # 检查数据完整性\")\n",
    "print(\"   • python -m data_check.check_length_distribution    # 检查长度分布\")\n",
    "print(\"   • python -m data_check.convert_custom_data          # 转换自定义数据\")\n",
    "print(\"   完整代码请查看: data_check/ 目录下的各个文件\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 训练脚本关键参数 (`scripts/base_train.py`)\n",
    "\n",
    "预训练脚本的关键配置和代码逻辑：\n",
    "\n",
    "**关键代码位置：**\n",
    "- 用户配置：```scripts/base_train.py```\n",
    "- 训练循环：```scripts/base_train.py```\n",
    "- 优化器设置：```scripts/base_train.py```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练脚本关键配置和代码\n",
    "# 完整代码请查看: scripts/base_train.py\n",
    "\n",
    "print(\"📄 预训练脚本关键配置：\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 用户配置\n",
    "# 完整代码：scripts/base_train.py (第 28-56 行)\n",
    "print(\"\"\"\n",
    "# 用户配置\n",
    "depth = 20                    # Transformer 深度\n",
    "max_seq_len = 2048            # 最大上下文长度\n",
    "device_batch_size = 32        # 每个 GPU 的批次大小\n",
    "total_batch_size = 524288     # 总批次大小（token 数）\n",
    "\n",
    "# 训练长度（三选一，按优先级）\n",
    "num_iterations = -1           # 明确的迭代次数（-1 = 禁用）\n",
    "target_flops = -1.0           # 目标 FLOPs（-1 = 禁用）\n",
    "target_param_data_ratio = 20 # Chinchilla 定律：数据token数 = 参数量 × 20\n",
    "\n",
    "# 优化器配置\n",
    "embedding_lr = 0.2            # 嵌入层学习率（Adam）\n",
    "unembedding_lr = 0.004        # 输出层学习率（Adam）\n",
    "matrix_lr = 0.02              # 矩阵参数学习率（Muon）\n",
    "grad_clip = 1.0               # 梯度裁剪值（0.0 = 禁用）\n",
    "\n",
    "# 评估配置\n",
    "eval_every = 250              # 每 250 步评估一次验证集 loss\n",
    "core_metric_every = 2000      # 每 2000 步评估一次 CORE 指标\n",
    "sample_every = 2000           # 每 2000 步采样一次\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# 训练循环核心逻辑\n",
    "# 完整代码：scripts/base_train.py (第 172-304 行)\n",
    "print(\"\"\"\n",
    "# 训练循环核心逻辑\n",
    "for step in range(num_iterations + 1):\n",
    "    # 1. 评估验证集 loss（定期）\n",
    "    if step % eval_every == 0:\n",
    "        model.eval()\n",
    "        val_bpb = evaluate_bpb(model, val_loader, eval_steps, token_bytes)\n",
    "        model.train()\n",
    "    \n",
    "    # 2. 单次训练步骤\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        # 前向传播\n",
    "        with autocast_ctx:\n",
    "            loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps  # 梯度累积归一化\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 预取下一批数据（异步）\n",
    "        x, y = next(train_loader)\n",
    "    \n",
    "    # 3. 梯度裁剪\n",
    "    if grad_clip > 0.0:\n",
    "        torch.nn.utils.clip_grad_norm_(orig_model.parameters(), grad_clip)\n",
    "    \n",
    "    # 4. 更新学习率\n",
    "    lrm = get_lr_multiplier(step)\n",
    "    for opt in optimizers:\n",
    "        for group in opt.param_groups:\n",
    "            group[\"lr\"] = group[\"initial_lr\"] * lrm\n",
    "    \n",
    "    # 5. 优化器步进\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    \n",
    "    # 6. 清理梯度\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # 7. 保存检查点（最后一步）\n",
    "    if step == num_iterations:\n",
    "        save_checkpoint(checkpoint_dir, step, orig_model.state_dict(), ...)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n💡 运行命令: torchrun --standalone --nproc_per_node=8 -m scripts.base_train --depth=20\")\n",
    "print(\"   关键参数说明：\")\n",
    "print(\"   • depth: 模型深度，决定参数量\")\n",
    "print(\"   • target_param_data_ratio: Chinchilla 定律比例（默认 20）\")\n",
    "print(\"   • device_batch_size: 每个 GPU 的批次大小\")\n",
    "print(\"   • total_batch_size: 所有 GPU 的总批次大小（token 数）\")\n",
    "print(\"   完整代码请查看: scripts/base_train.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 分词器评估代码 (`scripts/tok_eval.py`)\n",
    "\n",
    "评估分词器的压缩率，并与 GPT-2/GPT-4 分词器对比：\n",
    "\n",
    "**关键代码位置：**\n",
    "- 评估逻辑：```scripts/tok_eval.py```\n",
    "- 测试文本：```scripts/tok_eval.py```\n",
    "- 对比函数：```scripts/tok_eval.py```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词器评估代码\n",
    "# 完整代码请查看: scripts/tok_eval.py\n",
    "\n",
    "print(\"📄 分词器评估代码：\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 评估分词器压缩率的核心逻辑\n",
    "# 完整代码：scripts/tok_eval.py (第 152-190 行)\n",
    "print(\"\"\"\n",
    "# 评估分词器的压缩率\n",
    "from nanochat.tokenizer import get_tokenizer, RustBPETokenizer\n",
    "\n",
    "# 测试文本（新闻、代码、数学等不同类型）\n",
    "# 完整代码中包含了更多类型的测试文本\n",
    "test_texts = [\n",
    "    (\"news\", news_text),      # 新闻文本\n",
    "    (\"code\", code_text),      # 代码文本\n",
    "    (\"math\", math_text),      # 数学公式\n",
    "    (\"fwe-train\", train_text), # 训练数据\n",
    "]\n",
    "\n",
    "# 对比不同分词器\n",
    "tokenizers = {}\n",
    "tokenizers[\"gpt2\"] = RustBPETokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizers[\"gpt4\"] = RustBPETokenizer.from_pretrained(\"cl100k_base\")\n",
    "tokenizers[\"ours\"] = get_tokenizer()\n",
    "\n",
    "results = {}\n",
    "\n",
    "for tokenizer_name, tokenizer in tokenizers.items():\n",
    "    vocab_sizes[tokenizer_name] = tokenizer.get_vocab_size()\n",
    "    results[tokenizer_name] = {}\n",
    "    \n",
    "    for name, text in test_texts:\n",
    "        # 编码文本\n",
    "        encoded = tokenizer.encode(text)\n",
    "        decoded = tokenizer.decode(encoded)\n",
    "        assert decoded == text  # 验证可逆性\n",
    "        \n",
    "        # 计算压缩率（字节数 / token数）\n",
    "        encoded_bytes = len(text.encode('utf-8'))\n",
    "        ratio = encoded_bytes / len(encoded)\n",
    "        \n",
    "        results[tokenizer_name][name] = {\n",
    "            'bytes': encoded_bytes,\n",
    "            'tokens': len(encoded),\n",
    "            'ratio': ratio\n",
    "        }\n",
    "\n",
    "# 打印对比结果（详细表格）\n",
    "print_comparison(\"GPT-2\", results['gpt2'], results['ours'], test_texts)\n",
    "print_comparison(\"GPT-4\", results['gpt4'], results['ours'], test_texts)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n💡 运行命令: python -m scripts.tok_eval\")\n",
    "print(\"   这会评估分词器在不同类型文本上的压缩率\")\n",
    "print(\"   并与 GPT-2、GPT-4 的分词器进行对比\")\n",
    "print(\"   完整代码请查看: scripts/tok_eval.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义模型参数量（单位：百万）\n",
    "my_model_params = 100  # 修改这里！\n",
    "\n",
    "result = calculate_data_requirement(my_model_params)\n",
    "\n",
    "print(f\"\\n🎯 您的模型（{my_model_params}M 参数）需要：\\n\")\n",
    "print(f\"   Token 数量：{result['tokens']}\")\n",
    "print(f\"   字符数量：{result['chars']}\")\n",
    "print(f\"   数据分片：{result['shards']} 个\")\n",
    "print(f\"   磁盘空间：{result['disk']}\")\n",
    "print(\"\\n下载命令：\")\n",
    "print(f\"   python -m nanochat.dataset -n {result['shards']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <a id=\"检查清单\"></a>8. 完整流程检查清单\n",
    "\n",
    "准备好数据了吗？对照这个清单检查："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def check_data_readiness():\n",
    "    \"\"\"检查数据准备情况\"\"\"\n",
    "    \n",
    "    print(\"\\n🔍 数据准备状态检查\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    checks = []\n",
    "    \n",
    "    # 1. 检查预训练数据\n",
    "    base_data_dir = Path.home() / \".cache\" / \"nanochat\" / \"base_data\"\n",
    "    if base_data_dir.exists():\n",
    "        parquet_files = list(base_data_dir.glob(\"*.parquet\"))\n",
    "        if len(parquet_files) >= 8:\n",
    "            checks.append((\"✅\", f\"预训练数据：找到 {len(parquet_files)} 个分片\"))\n",
    "        else:\n",
    "            checks.append((\"⚠️\", f\"预训练数据：只有 {len(parquet_files)} 个分片（建议至少 8 个）\"))\n",
    "    else:\n",
    "        checks.append((\"❌\", \"预训练数据：未下载\"))\n",
    "    \n",
    "    # 2. 检查分词器\n",
    "    tokenizer_dir = Path.home() / \".cache\" / \"nanochat\" / \"tokenizer\"\n",
    "    if tokenizer_dir.exists() and list(tokenizer_dir.glob(\"*.model\")):\n",
    "        checks.append((\"✅\", \"分词器：已训练\"))\n",
    "    else:\n",
    "        checks.append((\"⚠️\", \"分词器：未训练（需要运行 tok_train）\"))\n",
    "    \n",
    "    # 3. 检查磁盘空间\n",
    "    cache_dir = Path.home() / \".cache\"\n",
    "    if cache_dir.exists():\n",
    "        try:\n",
    "            stat = shutil.disk_usage(cache_dir)\n",
    "            free_gb = stat.free / (1024**3)\n",
    "            if free_gb > 30:\n",
    "                checks.append((\"✅\", f\"磁盘空间：剩余 {free_gb:.1f} GB\"))\n",
    "            else:\n",
    "                checks.append((\"⚠️\", f\"磁盘空间：剩余 {free_gb:.1f} GB（建议至少 30GB）\"))\n",
    "        except:\n",
    "            checks.append((\"ℹ️\", \"磁盘空间：无法检测\"))\n",
    "    \n",
    "    # 4. 检查环境变量\n",
    "    if 'HF_ENDPOINT' in os.environ:\n",
    "        checks.append((\"✅\", f\"HuggingFace 镜像：{os.environ['HF_ENDPOINT']}\"))\n",
    "    else:\n",
    "        checks.append((\"ℹ️\", \"HuggingFace 镜像：未设置（国内用户建议设置）\"))\n",
    "    \n",
    "    # 显示结果\n",
    "    for status, msg in checks:\n",
    "        print(f\"{status} {msg}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 总结\n",
    "    ready_count = sum(1 for s, _ in checks if s == \"✅\")\n",
    "    total_count = len(checks)\n",
    "    \n",
    "    print(f\"\\n📊 就绪状态：{ready_count}/{total_count}\")\n",
    "    \n",
    "    if ready_count >= 2:  # 至少有数据和空间就算基本就绪\n",
    "        print(\"\\n🎉 数据基本准备完成，可以开始训练了！\")\n",
    "    else:\n",
    "        print(\"\\n💡 还有一些准备工作需要完成，请查看上面的提示\")\n",
    "\n",
    "# 运行检查\n",
    "check_data_readiness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🚀 下一步\n",
    "\n",
    "数据准备好了！接下来：\n",
    "\n",
    "### 1. 训练分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练分词器\n",
    "# ⚠️ 警告：这可能需要较长时间！\n",
    "\n",
    "# !python -m scripts.tok_train --max_chars=2000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 开始预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始预训练（需要 GPU）\n",
    "# ⚠️ 警告：这需要大量时间和计算资源！\n",
    "\n",
    "# !torchrun --standalone --nproc_per_node=8 -m scripts.base_train --depth=20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
